> æœ¬æ–‡ç”± [ç®€æ‚¦ SimpRead](http://ksria.com/simpread/) è½¬ç ï¼Œ åŸæ–‡åœ°å€ [segmentfault.com](https://segmentfault.com/a/1190000045501653) ![å¤´å›¾](/img/bVde5dF?spec=cover)

> **ç¼–è€…æŒ‰ï¼š** é¢å¯¹ Llama æ¨¡å‹å®¶æ—çš„æŒç»­æ›´æ–°ï¼Œæ‚¨æ˜¯å¦æƒ³è¦äº†è§£å®ƒä»¬ä¹‹é—´çš„å…³é”®åŒºåˆ«å’Œå®é™…æ€§èƒ½è¡¨ç°ï¼Ÿæœ¬æ–‡å°†æ¢è®¨ Llama ç³»åˆ—æ¨¡å‹çš„æ¶æ„æ¼”å˜ï¼Œæ¢³ç†äº† Llama æ¨¡å‹ä» 1.0 åˆ° 3.1 çš„å®Œæ•´æ¼”è¿›å†ç¨‹ï¼Œæ·±å…¥å‰–æäº†æ¯ä¸ªç‰ˆæœ¬çš„æŠ€æœ¯åˆ›æ–°ï¼Œè¿˜é€šè¿‡å®é™…å®éªŒå¯¹æ¯”äº† Llama 2 å’Œ Llama 3 åœ¨æ¨ç†é€Ÿåº¦ã€ç­”æ¡ˆé•¿åº¦å’Œç›¸å¯¹ç­”æ¡ˆè´¨é‡ï¼ˆRAQï¼‰ç­‰å…³é”®æŒ‡æ ‡ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚
> 
> æ ¹æ®æœ¬æ–‡ï¼Œ Llama æ¨¡å‹çš„æ¶æ„æ¼”å˜ä¸»è¦ç»å†äº†ä»¥ä¸‹ä¸‰ä¸ªé˜¶æ®µï¼š
> 
> *   Llama 1ï¼šåŸºäºåŸå§‹ Transformer æ¶æ„ï¼Œå¼•å…¥äº†é¢„å½’ä¸€åŒ–ã€RMSNormã€SwiGLU æ¿€æ´»å‡½æ•°å’Œæ—‹è½¬å¼ä½ç½®ç¼–ç ç­‰æ”¹è¿›ï¼Œæå‡äº†æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚
> *   Llama 2ï¼šåœ¨ Llama 1 çš„åŸºç¡€ä¸Šï¼Œå°†ä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•è‡³ 4096ï¼Œå¹¶å¼•å…¥äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA) æœºåˆ¶ï¼Œæœ‰æ•ˆé™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜éœ€æ±‚ï¼Œæå‡äº†æ¨ç†é€Ÿåº¦ã€‚
> *   Llama 3ï¼šè¿›ä¸€æ­¥å°† GQA åº”ç”¨äºå°å‹æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨æ›´é«˜æ•ˆçš„åˆ†è¯å™¨ TikTokenï¼Œæ‰©å¤§äº†è¯æ±‡è¡¨çš„æ•°é‡ï¼ŒåŒæ—¶å°†ä¸Šä¸‹æ–‡é•¿åº¦ç¿»å€ï¼Œå¹¶å¤§å¹…å¢åŠ äº†è®­ç»ƒæ•°æ®é‡ã€‚

**ä½œè€… | LuÃ­s Roque**

**ç¼–è¯‘ |Â å²³æ‰¬**

**01 Introduction**
-------------------

Meta å…¬å¸æ¨å‡ºäº†å…¶å¤§è¯­è¨€æ¨¡å‹ Llama çš„ä¸‰ä¸ªä¸»è¦ç‰ˆæœ¬ã€‚Llama åœ¨ 2023 å¹´åˆçš„é¦–åº¦äº®ç›¸ï¼Œä¸ºå¼€æºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç¤¾åŒºå¸¦æ¥äº†é‡å¤§çªç ´ã€‚Meta ä¸€ç›´é€šè¿‡åˆ†äº«æœ€æ–°çš„æ¨¡å‹ç‰ˆæœ¬ï¼Œä¸ºè¿™ä¸€ç¤¾åŒºè´¡çŒ®åŠ›é‡ã€‚

**åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦åŒºåˆ†â€œå¼€æ”¾å‹ï¼ˆopenï¼‰ LLMâ€ä¸â€œå¼€æºï¼ˆopen-sourceï¼‰ LLMâ€ã€‚** ä¼ ç»Ÿä¸Šï¼Œå¼€æºè½¯ä»¶ä¼šåœ¨ç‰¹å®šçš„å…¬å…±è®¸å¯è¯ä¸‹å…¬å¼€æºä»£ç ï¼Œå…è®¸ç”¨æˆ·ä½¿ç”¨å’Œä¿®æ”¹ã€‚åœ¨ LLM é¢†åŸŸï¼Œå¼€æ”¾å‹ LLM ä¼šå…¬å¼€æ¨¡å‹æƒé‡å’Œåˆå§‹ä»£ç ï¼Œè€Œå¼€æº LLM åˆ™ä¼šæ›´è¿›ä¸€æ­¥ï¼Œåœ¨å®½æ¾çš„è®¸å¯ä¸‹å…±äº«æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®ã€‚ç›®å‰ï¼ŒåŒ…æ‹¬ Meta çš„ Llama åœ¨å†…çš„å¤šæ•°æ¨¡å‹ï¼Œéƒ½å±äºå¼€æ”¾å‹ LLMï¼Œå› ä¸ºå®ƒä»¬å¹¶æœªå…¬å¼€ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚

**Llama ç»å†äº†ä¸‰æ¬¡é‡è¦çš„æ¶æ„æ›´æ–°ã€‚** ç‰ˆæœ¬ 1 å¯¹åŸå§‹çš„ Transformer æ¶æ„è¿›è¡Œäº†å¤šé¡¹æ”¹è¿›ã€‚ç‰ˆæœ¬ 2 åœ¨å¤§æ¨¡å‹ä¸­å¼•å…¥äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰æœºåˆ¶ã€‚ç‰ˆæœ¬ 3 å°†è¿™ä¸€æœºåˆ¶æ‰©å±•åˆ°äº†å°æ¨¡å‹ï¼ŒåŒæ—¶å¼•å…¥äº†æ›´é«˜æ•ˆçš„åˆ†è¯å™¨ï¼Œè¿˜æ‰©å¤§äº†è¯æ±‡é‡ã€‚ç‰ˆæœ¬ 3.1 å¹¶æœªå¯¹æ ¸å¿ƒæ¶æ„åšå‡ºè°ƒæ•´ï¼Œä¸»è¦çš„å˜åŒ–åœ¨äºè®­ç»ƒæ•°æ®çš„æ¸…æ´—ã€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ä»¥åŠå¯¹æ›´å¤šè¯­è¨€çš„æ”¯æŒã€‚

æœ¬æ–‡æ¢è®¨äº† Llama çš„æ¶æ„æ¼”å˜ï¼Œç€é‡ä»‹ç»å…¶ä¸»è¦è¿›æ­¥åŠå…¶å¯¹ LLM æœªæ¥å‘å±•çš„å½±å“ã€‚æ–‡ç« æœ€åé€šè¿‡ä¸€ä¸ªå®éªŒå¯¹ Llama 2 å’Œ Llama 3 è¿›è¡Œäº†æ¯”è¾ƒï¼Œä½¿ç”¨äº†æ¨ç†é€Ÿåº¦ã€ç­”æ¡ˆé•¿åº¦å’Œç›¸å¯¹ç­”æ¡ˆè´¨é‡ï¼ˆRAQï¼ŒRelative Answer Qualityï¼‰æ¡†æ¶[1]ç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚RAQ æ¡†æ¶æä¾›äº†ä¸€ä¸ªå®¢è§‚çš„è¯„åˆ†ç³»ç»Ÿï¼Œç”¨äºæ£€éªŒ LLM çš„å›ç­”å‡†ç¡®åº¦ï¼Œå¯¹äºè¯„ä¼°ç‰¹å®šåº”ç”¨åœºæ™¯å°¤ä¸ºæœ‰ç”¨ã€‚

![](/img/remote/1460000045501655)

Figure 1: Llama family (image by author with DALL-E)

**02 Llama: A Family of Open LLMs**
-----------------------------------

### **2.1 Llama 1ï¼šè¯¥ç³»åˆ—é¦–ä¸ªæ¨¡å‹é—®ä¸–**

Llama ç³»åˆ—çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ŒLlama 1 [2]ï¼Œæ˜¯å»ºç«‹åœ¨ Vaswani ç­‰äººåœ¨ 2017 å¹´æå‡ºçš„ç¼–ç å™¨-è§£ç å™¨ Transformer æ¶æ„ä¹‹ä¸Šçš„[3]ã€‚è¯¥æ¶æ„æ›¾æ˜¯ NLP é¢†åŸŸçš„é‡å¤§åˆ›æ–°ï¼Œå¹¶ä¸”è‡³ä»Šä»æ˜¯ LLM æ¨¡å‹çš„åŸºç¡€æ¶æ„ã€‚

Llama 1 åœ¨å…¶æ ¸å¿ƒè®¾è®¡ä¸­é‡‡çº³äº†è¿™ä¸€æ¶æ„ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†å¤šé¡¹ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ï¼š

**é¢„å½’ä¸€åŒ–æŠ€æœ¯**

å€Ÿé‰´äº† GPT3 [4]æ¶æ„ä¸­æé«˜è®­ç»ƒç¨³å®šæ€§çš„æ–¹æ³•ï¼ŒLlama 1 ä¹Ÿé‡‡ç”¨äº†å¯¹æ¯ä¸ª Transformer å­å±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–çš„ç­–ç•¥ï¼Œè€Œä¸ä»…ä»…æ˜¯å¯¹è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå…·ä½“ç»†èŠ‚å¦‚å›¾ 2 æ‰€ç¤ºã€‚

![](/img/remote/1460000045501656)

å›¾ 2ï¼šåŸå§‹ Transformer æ¶æ„ä¸ Llama 1 æ¶æ„çš„ä¸åŒä¹‹å¤„ï¼Œç‰¹åˆ«æ˜¯åœ¨ Transformer å­å±‚ä¸­ï¼Œå¯¹æ¯ä¸ªè¾“å…¥éƒ½è¿›è¡Œäº†å½’ä¸€åŒ–å¤„ç†ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰

æ­¤å¤–ï¼ŒLlama 1 è¿˜é‡‡ç”¨äº† RMSNorm [5] æ¥æ›¿ä»£ä¼ ç»Ÿçš„ LayerNorm å‡½æ•°ï¼Œè¿™ä¸€æ”¹å˜åœ¨ä¿æŒè®­ç»ƒç¨³å®šæ€§å’Œæå‡æ¨¡å‹æ”¶æ•›é€Ÿåº¦çš„åŒæ—¶ï¼Œå¤§å¹…æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚

RMSNorm ä¹‹æ‰€ä»¥èƒ½æ›´é«˜æ•ˆï¼Œæ˜¯å› ä¸ºå…¶åˆ›é€ è€…å‘ç° LayerNorm çš„ä¼˜åŠ¿åœ¨äº rescaling invarianceï¼ˆè¯‘è€…æ³¨ï¼šæŒ‡çš„æ˜¯å½’ä¸€åŒ–è¿‡ç¨‹èƒ½å¤Ÿé€‚åº”è¾“å…¥æ•°æ®çš„ç¼©æ”¾ï¼Œä½¿å¾—ç½‘ç»œå¯¹è¿™ç§ç¼©æ”¾ä¸æ•æ„Ÿã€‚ï¼‰ï¼Œè€Œé recentering invarianceï¼ˆè¯‘è€…æ³¨ï¼šå¦‚æœè¾“å…¥æ•°æ®çš„å‡å€¼å‘ç”Ÿäº†å˜åŒ–ï¼Œä½†æ•°æ®çš„åˆ†å¸ƒå½¢çŠ¶å’ŒèŒƒå›´ä¿æŒä¸å˜ï¼Œé‚£ä¹ˆå…·æœ‰ recentering invariance çš„ç®—æ³•æˆ–å‡½æ•°çš„è¾“å‡ºåº”è¯¥ä¸å—å½±å“ã€‚ï¼‰ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œä»–ä»¬çœç•¥äº†å½’ä¸€åŒ–è¿‡ç¨‹ä¸­çš„å‡å€¼è®¡ç®—ï¼Œä½¿å¾—ç®—æ³•æ›´åŠ ç®€æ´ï¼Œè€Œæ•ˆæœä¸å‡ï¼Œä¸”è¿ç®—æ•ˆç‡æ˜¾è‘—æå‡ã€‚

![](/img/remote/1460000045501657)

å›¾ 3ï¼šå±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰ä¸å‡æ–¹æ ¹å½’ä¸€åŒ–ï¼ˆRMSNormï¼‰ä¹‹é—´çš„æ–¹ç¨‹å·®å¼‚ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰

**SwiGLU æ¿€æ´»å‡½æ•°**

åœ¨æ¿€æ´»å‡½æ•°çš„é€‰æ‹©ä¸Šï¼Œç ”ç©¶è€…ä»¬é‡‡ç”¨äº† SwiGLU [6] å‡½æ•°æ¥æ›¿ä»£ä¼ ç»Ÿçš„ ReLU å‡½æ•°ï¼Œè¿™ä¸€æ”¹å˜æ—¨åœ¨æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸¤è€…çš„æ ¸å¿ƒå·®å¼‚åœ¨äºï¼š

*   ReLU å‡½æ•°ä¼šå°†æ‰€æœ‰è´Ÿæ•°è¾“å…¥ç›´æ¥å½’é›¶ï¼Œè€Œæ­£æ•°è¾“å…¥åˆ™ä¿æŒä¸å˜ã€‚
*   ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**SwiGLU å‡½æ•°å«æœ‰ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•° Î²ï¼Œèƒ½å¤Ÿè°ƒèŠ‚å‡½æ•°çš„æ’å€¼ç¨‹åº¦ã€‚** éšç€ Î² å€¼çš„å¢å¤§ï¼ŒSwiGLU çš„è¡Œä¸ºå°†é€æ¸æ¥è¿‘ ReLUï¼Œè¿™ä¸€ç‚¹å¦‚å›¾ 4 æ‰€ç¤ºã€‚

![](/img/remote/1460000045501658)

å›¾ 4ï¼šReLU ä¸ SwiGLU åœ¨ä¸åŒ Î² å€¼ä¸‹çš„è¡Œä¸ºå¯¹æ¯”ï¼Œå¯ä»¥çœ‹åˆ°å½“ Î² è¾¾åˆ° 100 æ—¶ï¼Œä¸¤è€…çš„æ›²çº¿è¶‹äºä¸€è‡´ã€‚

**æ—‹è½¬å¼ä½ç½®ç¼–ç ï¼ˆRotary Positional Embeddingsï¼‰**

åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œä½ç½®ç¼–ç èµ·åˆ°äº†è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œè¿™æ˜¯å› ä¸º Transformer æ¶æ„æœ¬èº«ä¸åŒºåˆ†å•è¯çš„é¡ºåºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ**å¦‚æœæ²¡æœ‰ä½ç½®ç¼–ç çš„è¾…åŠ©ï¼ŒTransformer ä¼šå°†å•è¯é¡ºåºä¸åŒä½†å•è¯ç›¸åŒçš„ä¸¤ä¸ªå¥å­è§†ä¸ºç›¸åŒçš„å¥å­ã€‚** ä¾‹å¦‚ï¼Œå¦‚æœæ²¡æœ‰ä½ç½®ç¼–ç ï¼Œä¸‹é¢ä¸¤ä¸ªå¥å­çš„å«ä¹‰ Transformer å°†æ— æ³•åŒºåˆ†ï¼š

> Sentence 1: Llama 2 is better than Llama 1 Sentence 2: Llama 1 is better than Llama 2
> 
> å¥å­1ï¼šLlama 2çš„æ€§èƒ½ä¼˜äºLlama 1ã€‚å¥å­2ï¼šLlama 1çš„æ€§èƒ½ä¼˜äºLlama 2ã€‚

åœ¨è®ºæ–‡[3]ä¸­ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°å®ç°çš„ç»å¯¹ä½ç½®ç¼–ç ï¼ˆAbsolute Positional Embeddingsï¼‰ã€‚**åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç¼–ç ï¼ˆpositional embeddingï¼‰ï¼Œå®ƒä»¬ä¸è¯å‘é‡ç›¸åŠ ï¼Œä»è€Œç¡®ä¿å³ä½¿å•è¯ç›¸åŒï¼Œä¸åŒé¡ºåºçš„å¥å­ä¹Ÿèƒ½è¡¨è¾¾ä¸åŒçš„æ„æ€ã€‚**

ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å¥å­ä¸­çš„å•è¯æ˜¯ç”¨ä¸€ç»´å‘é‡è€Œä¸æ˜¯å¤šç»´å‘é‡æ¥ç¼–ç çš„ã€‚å¦‚å›¾ 5 æ‰€ç¤ºï¼Œåœ¨è¯å‘é‡ä¸­ï¼Œâ€œ1â€å’Œâ€œ2â€çš„è¡¨ç¤ºå€¼æ˜¯ç›¸åŒçš„ã€‚ä½†æ˜¯ï¼Œåœ¨åŠ å…¥äº†ä½ç½®ç¼–ç ä¹‹åï¼Œå®ƒä»¬çš„è¡¨ç¤ºå€¼å°±å˜å¾—ä¸åŒäº†ï¼ˆåˆ†åˆ«ä»0.88å˜ä¸º1.04ï¼Œä»¥åŠä»0.26å˜ä¸º0.1ï¼‰ã€‚

![](/img/remote/1460000045501659)

å›¾ 5ï¼šç»å¯¹ä½ç½®ç¼–ç (Absolute Positional Embeddings)(å›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰

**å°½ç®¡ç»å¯¹ä½ç½®ç¼–ç å·²ç»è§£å†³äº† Transformer ä¸åŒºåˆ†é¡ºåºçš„é—®é¢˜ï¼Œä½†å®ƒç”Ÿæˆçš„ä½ç½®ç¼–ç æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°åºåˆ—ä¸­å•è¯ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚** è¿™æ„å‘³ç€åœ¨æ¨¡å‹çœ‹æ¥ï¼Œä½ç½® 1 å’Œä½ç½® 2 ä¹‹é—´çš„ç›¸å…³æ€§ä¸ä½ç½® 1 å’Œä½ç½® 500 ä¹‹é—´çš„ç›¸å…³æ€§å¹¶æ— å·®å¼‚ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çŸ¥é“å®é™…æƒ…å†µå¹¶éå¦‚æ­¤ï¼Œå› ä¸ºåœ¨ä½ç½®ä¸Šæ›´æ¥è¿‘çš„å•è¯ï¼Œå…¶ç›¸å…³æ€§ç†è®ºä¸Šåº”è¯¥æ›´é«˜ã€‚

æ—‹è½¬å¼ä½ç½®ç¼–ç [7]ï¼ˆRoPEï¼‰èƒ½å¤Ÿè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œ**å®ƒé€šè¿‡å°†åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®è½¬æ¢æˆè¯åµŒå…¥çš„æ—‹è½¬å˜é‡æ¥æ¨¡æ‹Ÿå•è¯é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚** ä»¥å‰æ–‡çš„ â€œLlama 2 is better than Llama 1â€ ä¸ºä¾‹ï¼Œå‡è®¾è¯åµŒå…¥ç°åœ¨æ˜¯äºŒç»´çš„ã€‚é‚£ä¹ˆï¼Œâ€œbetter â€ä¸€è¯å°†ç”±åŸºäºå…¶ä½ç½® m (4) å’Œå¸¸æ•° Î¸ çš„åŸå§‹äºŒç»´å‘é‡çš„äºŒç»´æ—‹è½¬å‘é‡æ¥è¡¨ç¤ºã€‚

![](/img/remote/1460000045501660)

å›¾ 6ï¼šå±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ—‹è½¬å¼ä½ç½®ç¼–ç ï¼ˆRotary Positional Embeddingï¼‰å°†åŸå§‹å‘é‡è½¬æ¢ä¸ºæ–°çš„å‘é‡ã€‚è¿™ä¸€è½¬æ¢æ˜¯åŸºäºå‘é‡åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œm=4ï¼‰å’Œå¸¸æ•°Î¸æ¥è¿›è¡Œçš„ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰

é‡‡ç”¨è¿™ç§æ–¹å¼ï¼Œå³ä¾¿åœ¨åŸå¥ä¸­å¢åŠ æ›´å¤šè¯æ±‡ï¼Œå•è¯ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ä¹Ÿèƒ½å¾—åˆ°ä¿æŒã€‚æ¯”å¦‚ï¼Œåœ¨å¥å­ â€œThe LLM Llama 2 is better than Llama 1â€ ä¸­æ·»åŠ ä¸¤ä¸ªå•è¯ï¼Œå°½ç®¡â€œbetterâ€å’Œâ€œthanâ€çš„ä½ç½®ä»ï¼ˆ4å’Œ5ï¼‰å˜ä¸ºï¼ˆ6å’Œ7ï¼‰ï¼Œä½†ç”±äºæ—‹è½¬é‡ä¿æŒä¸€è‡´ï¼Œä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ˆå³å·¦å›¾ä¸­å‘é‡çš„ç‚¹ç§¯ä¸å³å›¾ä¸­çš„ç‚¹ç§¯ç›¸åŒï¼‰ä¾æ—§ä¸å˜ã€‚

![](/img/remote/1460000045501661)

å›¾ 7ï¼šæ—‹è½¬å¼ä½ç½®ç¼–ç ç»´æŒ tokens é—´ç›¸å¯¹è·ç¦»çš„èƒ½åŠ›ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰

### **2.2 Llama 2ï¼šLlama 1 çš„å‡çº§ç‰ˆ**

Llama 2 [8] ä¿ç•™äº† Llama 1 å¯¹åŸå§‹ Transformer æ¶æ„æ‰€åšçš„æ‰€æœ‰æ”¹åŠ¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿˜å°†å¤„ç†ä¸Šä¸‹æ–‡çš„é•¿åº¦æ‰©å±•è‡³ 4096ï¼Œç›¸è¾ƒäºä¹‹å‰çš„ 2048ï¼Œç¿»äº†ä¸€ç•ªã€‚åŒæ—¶ï¼Œå¯¹äº 34B å’Œ 70B è¿™æ ·çš„å¤§å‹æ¨¡å‹ï¼ŒLlama 2 ä½¿ç”¨ Grouped-Query Attention (GQA) [10] å–ä»£äº†ä¼ ç»Ÿçš„ Multi-Head Attention (MHA) [9]ã€‚

ç”±äºéœ€è¦å¤§é‡å†…å­˜æ¥åŠ è½½æ‰€æœ‰çš„æ³¨æ„åŠ›å¤´çš„ queriesã€keys å’Œ values ï¼Œ**MHA æˆä¸ºäº† Transformer çš„æ€§èƒ½ç“¶é¢ˆ**ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š

1.  Multi-Query Attention [9]ï¼ˆMQAï¼‰é€šè¿‡åœ¨æ³¨æ„åŠ›å±‚ä½¿ç”¨å•ä¸€çš„é”®å’Œå€¼å¤´ï¼ˆkey and valueï¼‰ï¼Œé…åˆå¤šä¸ªæŸ¥è¯¢å¤´ï¼ˆquery headsï¼‰æ¥å¤§å¹…é™ä½å†…å­˜éœ€æ±‚ã€‚ä½†è¿™ç§åšæ³•å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„è´¨é‡ï¼Œå¹¶å¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸ç¨³å®šï¼Œå› æ­¤åƒ T5 è¿™æ ·çš„å…¶ä»–å¼€æºå¤§è¯­è¨€æ¨¡å‹å¹¶æœªé‡‡ç”¨æ­¤æ–¹æ³•ã€‚
2.  GQA åˆ™é‡‡ç”¨äº†ä¸€ç§æŠ˜ä¸­æ–¹æ¡ˆï¼Œå®ƒå°†æŸ¥è¯¢å€¼ï¼ˆquery valuesï¼‰åˆ†ä¸º G ç»„ï¼ˆGQA-Gï¼‰ï¼Œæ¯ç»„å…±äº«ä¸€ä¸ªé”®å’Œå€¼å¤´ï¼ˆkey and value headï¼‰ã€‚å¦‚æœ GQA çš„ç»„æ•°ä¸º 1ï¼ˆGQA-1ï¼‰ï¼Œåˆ™ç›¸å½“äº MQAï¼Œæ‰€æœ‰æŸ¥è¯¢ï¼ˆqueriesï¼‰éƒ½é›†ä¸­åœ¨ä¸€ç»„ï¼›è€Œå¦‚æœç»„æ•°ç­‰äºå¤´æ•°ï¼ˆGQA-Hï¼‰ï¼Œåˆ™ä¸ MHA ç›¸å½“ï¼Œæ¯ä¸ªæŸ¥è¯¢ï¼ˆqueryï¼‰è‡ªæˆä¸€ç»„ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†æ¯ä¸ªæŸ¥è¯¢ï¼ˆqueryï¼‰ç»„ä¸­çš„é”®å’Œå€¼å¤´ï¼ˆkeys and valuesï¼‰æ•°é‡ï¼Œä»è€Œç¼©å°äº†é”®å€¼ç¼“å­˜çš„å¤§å°ï¼Œå‡å°‘äº†éœ€è¦åŠ è½½çš„æ•°æ®é‡ã€‚**ä¸ MQA ç›¸æ¯”ï¼Œè¿™ç§æ›´ä¸ºæ¸©å’Œçš„ç¼©å‡æ–¹å¼åœ¨æå‡æ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œä¹Ÿé™ä½äº†è§£ç è¿‡ç¨‹ä¸­çš„å†…å­˜éœ€æ±‚ï¼Œä¸”æ¨¡å‹è´¨é‡æ›´æ¥è¿‘ MHAï¼Œé€Ÿåº¦å‡ ä¹ä¸ MQA æŒå¹³ã€‚**

![](/img/remote/1460000045501662)

å›¾ 8ï¼šMHAã€GQA å’Œ MQA æ–¹æ³•æ¦‚è§ˆï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰

### **2.3 Llama 3: Size and Tokenization**

Llama 3 [11] å°†å¤„ç†ä¸Šä¸‹æ–‡çš„é•¿åº¦ä» 4096 æ‰©å±•è‡³ 8192ï¼Œå¹¶å°† GQA ä½¿ç”¨åˆ°äº†è¾ƒå°è§„æ¨¡çš„æ¨¡å‹ï¼ˆ8Bï¼‰ã€‚åŒæ—¶ï¼Œç ”ç©¶è€…ä»¬è¿˜å°†åˆ†è¯å·¥å…·ä» Sentence Piece [12] æ›´æ¢ä¸º OpenAI æ¨¡å‹æ‰€é‡‡ç”¨çš„ TikToken [13]ã€‚å› ä¸ºæ–°çš„è¯æ±‡è¡¨å®¹é‡å¢åŠ åˆ°äº† 128k ä¸ª tokensï¼Œè¾ƒä¹‹å‰çš„ 32k æœ‰äº†å¤§å¹…æå‡ï¼Œè¿™ä¸€å˜æ›´æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚

**è¿™ä¸¤ç§åˆ†è¯å·¥å…·çš„ä¸»è¦å·®å¼‚åœ¨äºï¼Œåœ¨è¾“å…¥çš„ tokens å·²ç»å­˜åœ¨äºè¯æ±‡è¡¨ä¸­æ—¶ï¼ŒTikToken ä¼šè·³è¿‡å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰** **[14]** **çš„åˆå¹¶è§„åˆ™ã€‚** ä¾‹å¦‚ï¼Œå¦‚æœâ€œgeneratingâ€è¿™ä¸ªè¯å·²ç»åœ¨è¯æ±‡è¡¨ä¸­äº†ï¼Œé‚£ä¹ˆå®ƒå°†ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„ token è¿”å›ï¼Œè€Œä¸æ˜¯å°†å…¶æ‹†åˆ†ä¸ºâ€œgeneratingâ€å’Œâ€œingâ€è¿™ä¸¤ä¸ªæœ€å°å•å…ƒçš„ tokens ã€‚

### **2.4 Llama 3.1**

åœ¨ 2024 å¹´ 7 æœˆå‘å¸ƒçš„ Llama 3.1ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆ128K tokensï¼‰çš„æ˜¾è‘—æå‡ï¼Œå¹¶æ–°å¢äº†å¯¹ 8 ç§è¯­è¨€çš„æ”¯æŒã€‚æ­¤æ¬¡å‘å¸ƒç‰ˆæœ¬çš„ä¸€ä¸ªé‡è¦äº®ç‚¹æ˜¯æ›´å¤§çš„ Llama 3.1 405B æ¨¡å‹ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œå¼€æ”¾å¼çš„ LLMsï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰é€šå¸¸æ¨¡å‹è§„æ¨¡éƒ½ä½äº 100 Bã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸‹è¡¨ä¸­æ€»ç»“ä¸€ä¸‹ Llama æ¨¡å‹çš„æ¼”å˜æƒ…å†µï¼š

![](/img/remote/1460000045501663)

è¡¨ 1ï¼šæ¯”è¾ƒ Llama æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡é•¿åº¦ã€è¯æ±‡è¡¨å¤§å°ã€è®­ç»ƒæ•°æ®é›†å¤§å°ä»¥åŠæ”¯æŒè¯­è¨€æ•°é‡æ–¹é¢çš„æ¼”å˜ã€‚

**03 Llama 2 ä¸ Llama 3ï¼šæ¨¡å‹æ¯”è¾ƒ**
-----------------------------

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°† Llama2 å’Œ Llama 3 æ¨¡å‹åœ¨ SQuAD æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚SQuAD æ˜¯ä¸€ä¸ªé‡‡ç”¨ CC BY-SA 4.0 è®¸å¯åè®®çš„é—®ç­”æ•°æ®é›†ï¼ˆ[https://huggingface.co/datasets/rajpurkar/squad](https://link.segmentfault.com/?enc=5xk4zcseoBJhB78ZKbXciQ%3D%3D.lm1N8V8%2FHhFhKDoEt3QEOIRrZHwqFRu40Y1QxxYMm8Slli1ZkevpuA69BPhgv7mp)ï¼‰ã€‚è¯¥é˜…è¯»ç†è§£æ•°æ®é›†ï¼ˆreading comprehension datasetï¼‰ç”±ä¸€ç³»åˆ—ç»´åŸºç™¾ç§‘æ–‡ç« çš„é—®é¢˜ç»„æˆã€‚æ¨¡å‹éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œæ£€ç´¢å‡ºé—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆã€‚å¯¹äºæœ¬æ¬¡æ¨¡å‹æ¯”è¾ƒï¼Œæ•°æ®é›†ä¸­æœ‰ä¸‰ä¸ªè¾ƒä¸ºé‡è¦çš„å­—æ®µï¼š

*   é—®é¢˜ï¼ˆquestionï¼‰â€”â€”æ¨¡å‹éœ€è¦å›ç­”çš„é—®é¢˜ã€‚
*   ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰â€”â€”æ¨¡å‹éœ€è¦ä»ä¸­æå–ç­”æ¡ˆçš„èƒŒæ™¯ä¿¡æ¯ã€‚
*   ç­”æ¡ˆï¼ˆanswersï¼‰â€”â€”é—®é¢˜çš„æ–‡æœ¬ç­”æ¡ˆã€‚

è¯„ä¼°è¿‡ç¨‹å°†åŒ…æ‹¬ä¸‰ä¸ªé‡åŒ–æŒ‡æ ‡ï¼š**ç¬¬ä¸€ä¸ªæ˜¯è¯„ä¼°æ¨ç†é€Ÿåº¦ï¼Œç¬¬äºŒä¸ªæ˜¯ç¡®å®šç­”æ¡ˆé•¿åº¦ï¼Œç¬¬ä¸‰ä¸ªæ˜¯è¯„ä¼°å‡†ç¡®æ€§ã€‚** å¯¹äºå‡†ç¡®æ€§çš„è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨ RAQ [1]ã€‚RAQ é€šè¿‡ä¸€ä¸ªç‹¬ç«‹çš„ LLM å¯¹ Llama 2 å’Œ Llama 3 çš„ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œæ’åºçš„ä¾æ®æ˜¯å®ƒä»¬ä¸çœŸå®ç­”æ¡ˆçš„æ¥è¿‘ç¨‹åº¦ã€‚

æˆ‘ä»¬é¦–å…ˆä¸‹è½½è¿™ä¸¤ä¸ªæ¨¡å‹çš„ .gguf æ ¼å¼æ–‡ä»¶ï¼Œä»¥ä¾¿èƒ½å¤Ÿåœ¨ CPU ä¸Šè¿è¡Œå®ƒä»¬ï¼Œå¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨ model/ æ–‡ä»¶å¤¹ä¸‹ã€‚

æˆ‘ä»¬ä½¿ç”¨äº†æ¯ä¸ªæ¨¡å‹çš„ instruct ç‰ˆæœ¬ï¼Œå¹¶è¿›è¡Œäº† 4-bit é‡åŒ–ï¼š

*   nous-hermes-Llama-2-7b.Q4_K_M.ggufï¼Œæ¥è‡ª [https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGUF](https://link.segmentfault.com/?enc=MoLONJLHeKAlwuysmSViaA%3D%3D.Cz4Jx9jjAu6CyvCgVxvqZ6ekT7TZPmABAkEnbwpqXV0xbzcDMTxVQz31rA5ibp%2BwMxS7Tid8l5YLnakmYPkBew%3D%3D)
*   Meta-Llama-3-8B-Instruct-Q4_K_M.ggufï¼Œæ¥è‡ª [https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct-...](https://link.segmentfault.com/?enc=eUZGw75nbtWKkoD2%2BB5UHA%3D%3D.7QruKuTHkUQfMLcXNz9Aghi04Ku%2B9R%2FIx6uIRGee7WIIigoqF0xyAc4Q7vIMqmMOGwYZDlw8aqtX0CyT0MFPukdtUGMJPlzCUpaR1PQLSkw%3D)

åœ¨å®Œæˆä¸Šè¿°æ“ä½œä¹‹åï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä¼šå¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ï¼Œä»¥åŠæˆ‘ä»¬è‡ªå®šä¹‰çš„ä¸€ä¸ªç”Ÿæˆå™¨ã€‚è¿™ä¸ªç”Ÿæˆå™¨æ˜¯ä¸€ä¸ªå‡½æ•°æˆ–è€…ç±»ï¼Œå®ƒèƒ½å¤Ÿæ¥å—æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹ä½œä¸ºè¾“å…¥å‚æ•°ã€‚

![](/img/remote/1460000045501664)

è¿™ä¸ªç±»çš„ä½œç”¨æ˜¯ä» config.yaml é…ç½®æ–‡ä»¶ä¸­è½½å…¥æ¨¡å‹å‚æ•°ï¼Œè¿™äº›å‚æ•°çš„å…·ä½“è®¾ç½®åŒ…æ‹¬ï¼šè®¾å®šä¸Šä¸‹æ–‡é•¿åº¦ä¸º 1024ï¼Œè°ƒèŠ‚æ¨¡å‹è¿è¡Œçš„â€œtemperature â€ä¸º 0.7ï¼Œä»¥åŠé™åˆ¶è¾“å‡ºçš„æœ€å¤§ tokens æ•°ä¸º2000ã€‚

![](/img/remote/1460000045501665)

æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜æ„å»ºäº†ä¸€ä¸ªåŸºäº LangChain çš„æç¤ºè¯æ¨¡æ¿ã€‚è¿™ä¸ªæ¨¡æ¿çš„ä½œç”¨æ˜¯åœ¨å°†é—®é¢˜å’Œç›¸å…³ä¸Šä¸‹æ–‡æäº¤ç»™å¤§è¯­è¨€æ¨¡å‹ä¹‹å‰ï¼Œå¯¹å®ƒä»¬è¿›è¡Œæ ¼å¼åŒ–å¤„ç†ï¼Œä»¥ä¾¿è·å¾—æ›´å‡†ç¡®çš„å“åº”ã€‚

![](/img/remote/1460000045501666)

å‡½æ•° get_llm_response è´Ÿè´£æ¥æ”¶å·²åŠ è½½çš„å¤§è¯­è¨€æ¨¡å‹ã€ç›¸å…³ä¸Šä¸‹æ–‡ä»¥åŠé—®é¢˜ï¼Œå¹¶è¾“å‡ºæ¨¡å‹çš„å›ç­”ä»¥åŠä¸€ç³»åˆ—é‡åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚

![](/img/remote/1460000045501667)

è¯„ä¼°ç»“æŸåï¼Œæˆ‘ä»¬å°†å„é¡¹æŒ‡æ ‡è¿›è¡Œäº†å¯è§†åŒ–å±•ç¤ºï¼Œå¹¶å‘ç° **Llama 3 çš„é€Ÿåº¦æ¯” Llama 2 å¿«**ï¼Œå…¶å¹³å‡ç”Ÿæˆé€Ÿåº¦è¾¾åˆ°æ¯ç§’ 1.1 ä¸ªå•è¯ï¼Œè€Œ Llama 2 çš„ç”Ÿæˆé€Ÿåº¦ä»…ä¸ºæ¯ç§’ 0.25 ä¸ªå•è¯ã€‚åœ¨ç­”æ¡ˆé•¿åº¦æ–¹é¢ï¼Œ**Llama 3 è¾“å‡ºçš„ç­”æ¡ˆè¾ƒé•¿ï¼Œå¹³å‡ä¸º 70 ä¸ªå•è¯ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒLlama 2 7B çš„ç­”æ¡ˆå¹³å‡é•¿åº¦åªæœ‰ 15 ä¸ªå•è¯**ã€‚æ ¹æ®ç›¸å¯¹ç­”æ¡ˆè´¨é‡ï¼ˆRAQï¼ŒRelative Answer Qualityï¼‰è¯„ä¼°æ¡†æ¶ï¼Œ**Llama 3 åœ¨å¹³å‡æ’åä¸Šæ‹”å¾—å¤´ç­¹ï¼Œçº¦ä¸º 1.25ï¼Œè€Œ Llama 2 çš„è¡¨ç°åˆ™ç¨é€Šä¸€ç­¹ï¼Œå…¶å¹³å‡æ’åå¤§çº¦ä¸º 1.8ã€‚**

![](/img/remote/1460000045501668)

å›¾ 9ï¼šLlama 2 7B vs Llama 3 8Bï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰

è¡¨ 2 å±•ç¤ºäº†ä¸åŒè¯­è¨€æ¨¡å‹æ€§èƒ½çš„ Dunn äº‹åæ£€éªŒï¼ˆDunn post-hoc testï¼‰ç»“æœã€‚æ¯ä¸ªå•å…ƒæ ¼æ˜¾ç¤ºäº†ä¸¤ç§æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚æ˜¯å¦åœ¨ 5 %çš„æ˜¾è‘—æ€§æ°´å¹³ï¼ˆsignificance levelï¼‰ä¸Šå…·æœ‰ç»Ÿè®¡æ„ä¹‰ã€‚"Significant" æ„å‘³ç€å­˜åœ¨ç»Ÿè®¡ä¸Šçš„æ˜¾è‘—å·®å¼‚ï¼ˆpå€¼ä¸è¶…è¿‡0.05ï¼‰ï¼Œè€Œ "Not Significant" åˆ™æ„å‘³ç€æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ä¸å…·å¤‡ç»Ÿè®¡æ˜¾è‘—æ€§ï¼ˆpå€¼è¶…è¿‡0.05ï¼‰ã€‚æ ¹æ®æ£€éªŒç»“æœï¼ŒLlama 3 ä¸ Llama 2 åœ¨æ€§èƒ½ä¸Šçš„å·®å¼‚æ˜¯æ˜¾è‘—çš„ã€‚

![](/img/remote/1460000045501669)

![](/img/remote/1460000045501670)

è¡¨ 2ï¼šä¸åŒ LLM æ¨¡å‹æ€§èƒ½æ’åå·®å¼‚çš„æ˜¾è‘—æ€§åˆ†æ

æœ€åï¼Œä»å®šæ€§è§’åº¦ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸¤ç§æ¨¡å‹å¯¹æŸä¸€ç‰¹å®šé—®é¢˜çš„å›ç­”ï¼šâ€œWhat percentage of improvement over energy code requirements will be the goal of all new construction and renovations?â€ã€‚è¿™ä¸€é—®é¢˜åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å¾—å‡ºç­”æ¡ˆï¼Œä¸¤è€…å‡æ­£ç¡®åœ°å›ç­”äº†é—®é¢˜ã€‚

![](/img/remote/1460000045501672)

ç„¶åï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼ŒLlama 2 åœ¨å›ç­”è¿‡ç¨‹ä¸­å…ˆæ˜¯è¡¨ç¤ºç­”æ¡ˆä¸åœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­ï¼Œä½†æœ€ç»ˆå´åˆå¼•ç”¨äº†ä¸Šä¸‹æ–‡ä¸­çš„å†…å®¹æ¥ç»™å‡ºç­”æ¡ˆï¼Œæ˜¾å¾—å‰åçŸ›ç›¾ã€‚è€Œ Llama 3 åˆ™èƒ½å¤Ÿå‡†ç¡®åœ°ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œå¹¶ç®€æ´æ˜äº†åœ°ä½œå‡ºäº†æ­£ç¡®å›åº”ã€‚

![](/img/remote/1460000045501673)

**04 Conclusions**
------------------

Llama æ¨¡å‹åœ¨å‘å±•è¿‡ç¨‹ä¸­ä¸æ–­è¿›è¡Œæ”¹è¿›ï¼Œä½¿æ¨¡å‹åœ¨å¤„ç†è¯­è¨€ä»»åŠ¡æ—¶æ›´åŠ é«˜æ•ˆã€è¡¨ç°æ›´ä½³ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€‚åº”æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚ä»æœ€åˆçš„ Llama 1 å¼€å§‹ï¼Œå¼•å…¥äº†å¦‚ RMSNorm è¾“å…¥å½’ä¸€åŒ–å’Œæ›´å¹³æ»‘çš„æ¿€æ´»å‡½æ•°ç­‰åŸºç¡€æ€§æ”¹å˜ï¼Œåç»­çš„æ¯ä¸ªæ¨¡å‹ç‰ˆæœ¬éƒ½æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ”¹è¿›ã€‚

Llama 2 é€šè¿‡é‡‡ç”¨ GQA æé«˜æ¨ç†æ•ˆç‡ï¼Œå¯¹è¿™ä¸€æ–¹æ³•è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä¸º Llama 3 çš„è¿›ä¸€æ­¥æå‡é“ºå¹³é“è·¯ã€‚Llama 3 åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå°† GQA åº”ç”¨äºæ›´å°å‹çš„æ¨¡å‹ï¼Œé‡‡ç”¨äº†è¯æ±‡è¡¨æ•°é‡æ›´å¤§çš„é«˜æ•ˆåˆ†è¯å™¨ï¼Œå°†ä¸Šä¸‹æ–‡é•¿åº¦ç¿»å€ï¼Œå¹¶å¤§å¹…å¢åŠ äº†è®­ç»ƒæ•°æ®é‡ã€‚

Llama 3.1 ç‰ˆæœ¬å¼€å¯äº†æ–°çš„ç¯‡ç« ã€‚å®ƒå°†ä¸Šä¸‹æ–‡é•¿åº¦è¿›ä¸€æ­¥æ‰©å±•è‡³ 128K ä¸ª tokenï¼Œå¢åŠ äº†å¯¹æ›´å¤šè¯­è¨€çš„æ”¯æŒï¼Œå¹¶æ¨å‡ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¼€æ”¾å¼æ¨¡å‹ â€”â€” 405B æ¨¡å‹ã€‚

Llama æ¨¡å‹çš„è¿ç»­å‡çº§ï¼Œä½¿å¾—å®ƒä»¬åœ¨å„ç§åº”ç”¨åœºæ™¯ä¸­éƒ½å…·æœ‰å“è¶Šçš„é€‚åº”æ€§ã€‚è‡³ä»Šï¼ŒLlama æ¨¡å‹å·²ç´¯è®¡ä¸‹è½½è¶…è¿‡ 3 äº¿æ¬¡ï¼Œè€Œå°†å…¶é›†æˆåˆ°åˆ©ç”¨ç§æœ‰ LLM æŠ€æœ¯çš„æ•°åƒç§äº§å“ä¸­ï¼Œä»…ä»…æ˜¯ä¸€ä¸ªå¼€å§‹ã€‚é¢‡å…·è®½åˆºæ„å‘³çš„æ˜¯ï¼ŒLlama ç°åœ¨åœ¨æ¨åŠ¨å¼€æ”¾å¼ AI çš„å‘å±•é“è·¯ä¸Šèµ°åœ¨äº†å‰åˆ—ï¼Œå–ä»£äº†æ›¾ç»æ›´ä¸ºå¼€æ”¾çš„ OpenAI æ‰€å æ®çš„ä½ç½®ã€‚

**References**
--------------

[1] LuÃ­s Roque, Rafael Guedes. â€œResearch to Production: Relative Answer Quality (RAQ) and NVIDIA NIM.â€ Towards Data Science. Medium, 2024.

[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. â€œLlama: Open and Efficient Foundation Language Models.â€ arXiv preprint arXiv:2302.13971, 2023.

[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. â€œAttention Is All You Need.â€ arXiv preprint arXiv:1706.03762, 2017.

[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. â€œLanguage Models are Few-Shot Learners.â€ arXiv preprint arXiv:2005.14165, 2020.

[5] Biao Zhang, Rico Sennrich. â€œRoot Mean Square Layer Normalization.â€ arXiv preprint arXiv:1910.07467, 2019.

[6] Noam Shazeer. â€œGLU Variants Improve Transformer.â€ arXiv preprint arXiv:2002.05202, 2020.

[7] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. â€œRoFormer: Enhanced Transformer with Rotary Position Embedding.â€ arXiv preprint arXiv:2104.09864, 2021.

[8] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom. â€œLlama 2: Open Foundation and Fine-Tuned Chat Models.â€ arXiv preprint arXiv:2307.09288, 2023.

[9] Noam Shazeer. â€œFast Transformer Decoding: One Write-Head is All You Need.â€ arXiv preprint arXiv:1911.02150, 2019.

[10] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico LebrÃ³n, Sumit Sanghai. â€œGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.â€ arXiv preprint arXiv:2305.13245, 2023.

[11] Meta AI. â€œIntroducing Llama 3.â€ Meta AI Blog, 2024.

[12] Taku Kudo, John Richardson. â€œSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing.â€ arXiv preprint arXiv:1808.06226, 2018.

[13] OpenAI. â€œTikToken.â€ GitHub.

[14] Rico Sennrich, Barry Haddow, Alexandra Birch. â€œNeural Machine Translation of Rare Words with Subword Units.â€ arXiv preprint arXiv:1508.07909, 2015.

_Thanks for reading!_

_Hope you have enjoyed and learned new things from this blog!_

**_About the authors_**

**LuÃ­s Roque**

Serial entrepreneur and leader in the AI space. I develop AI products for businesses and invest in AI-focused startups.

[https://www.linkedin.com/in/luisbrasroque/](https://link.segmentfault.com/?enc=Zu2gllymO9zFcENfrrZMfQ%3D%3D.TbcuZt9DJ8AxuJWkPnIWQJk0nVc%2BithQxmbIcC2jwFsTolrkz45FEzHbvCHUJrSW)

**END**

**æœ¬æœŸäº’åŠ¨å†…å®¹ ğŸ»**

**â“æ–‡ç« æåˆ° Llama æ¨¡å‹é‡‡ç”¨äº†å¤šé¡¹æŠ€æœ¯ä¼˜åŒ–ï¼Œå¦‚ GQA å’Œé«˜æ•ˆåˆ†è¯å™¨ç­‰ã€‚ä½ è®¤ä¸ºåœ¨è¿™äº›ä¼˜åŒ–ä¸­ï¼Œå“ªä¸€é¡¹å¯¹æ¨¡å‹æ€§èƒ½æå‡æœ€å…³é”®ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ**

**åŸæ–‡é“¾æ¥ï¼š**

[https://towardsdatascience.com/the-evolution-of-llama-from-ll...](https://link.segmentfault.com/?enc=l9qgyE%2BY9r%2BKgQdb%2FwNGeA%3D%3D.hHcfJmBwWYA%2BpxzLKI3tccT3nbBLeCqpGbLbOUvEpys7zdrFepIRoV4nBilsbX0ePc0winvYwWSXt1jYb17oxcCJepVBaH2BJE9zOGyq9yuL0jnJgwxH5gGkFzaa4diA)

[![æå®¢è§‚ç‚¹](https://avatar-static.segmentfault.com/159/147/1591470715-67498d9667680_huge128)](/site/thinking)[llama](/t/llama)[llm](/t/llm)[generative-ai](/t/generative-ai)[äººå·¥æ™ºèƒ½](/t/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD)[![](https://avatar-static.segmentfault.com/187/562/1875620466-5df9fe086a004_small)ç¨‹åºå‘˜](/t/%E7%A8%8B%E5%BA%8F%E5%91%98)èµæ”¶è—åˆ†äº«é˜…è¯» 180[å‘å¸ƒäº 11 æœˆ 25 æ—¥](/a/1190000045501653/revision)

* * *

[![å¤´åƒ](https://avatar-static.segmentfault.com/399/358/3993582402-66a1a56283552_huge128)](/u/baihai_idp)[

##### Baihai_IDP

](/u/baihai_idp)134 å£°æœ›445 ç²‰ä¸

IDPæ˜¯AIè®­æ¨äº‘å¹³å°ï¼Œæ—¨åœ¨ä¸ºä¼ä¸šå’Œæœºæ„æä¾›ç®—åŠ›èµ„æºã€æ¨¡å‹æ„å»ºä¸æ¨¡å‹åº”ç”¨äºä¸€ä½“çš„å¹³å°è§£å†³æ–¹æ¡ˆï¼Œå¸®åŠ©ä¼ä¸šé«˜æ•ˆå¿«é€Ÿæ„å»ºä¸“å±AIåŠå¤§æ¨¡å‹ã€‚

å…³æ³¨ä½œè€…

* * *

Â« ä¸Šä¸€ç¯‡[AIåƒäººä¸€æ ·æ“æ§ç”µè„‘ï¼šå¤šæ¨¡æ€AI Agentså’Œå±å¹•äº¤äº’æ–°èŒƒå¼](/a/1190000045488751)ä¸‹ä¸€ç¯‡ Â»[é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹è¯„ä¼°ä½“ç³»æ¢æ](/a/1190000045517517)