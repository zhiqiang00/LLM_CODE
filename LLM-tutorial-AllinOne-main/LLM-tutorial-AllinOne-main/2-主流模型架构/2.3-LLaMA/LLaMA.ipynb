{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA\n",
    "LLaMA(大型语言模型 Meta AI) 是一系列先进的基础语言模型，参数范围从 70 亿到 650 亿。这些模型体积较小，但性能卓越，显著降低了实验新方法、验证他人工作的计算能力和资源需求，同时探索创新用例。\n",
    "\n",
    "Llama使用了数万亿token的数据进行训练，证明了使用公开数据集就能够训练出sota(state of art最先进)模型，使用了大量的无标注数据，使用到的数据主要如下：\n",
    "- 67.0% CommonCrawl\n",
    "- 15.0% C4\n",
    "- 4.5% GitHub\n",
    "- 4.5% Wikipedia 4.5% 维基百科\n",
    "- 4.5% Books 4.5% 书籍\n",
    "- 2.5% ArXiv\n",
    "- 2.0% StackExchange\n",
    "\n",
    "通过使用多样化的数据进行训练，`LLaMA-13B` 在大多数基准测试中优于 GPT-3（175B） ，而 `LLaMA-65B` 则与最佳模型 `Chinchilla-70B` 和 `PaLM-540B` 相当。\n",
    "\n",
    "1. 官方文档：[Llama documention](https://www.llama.com/docs/overview)\n",
    "2. HuggingFace官方文档：[LLaMA](https://huggingface.co/docs/transformers/main/en/model_doc/llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing_extensions import Self\n",
    "\n",
    "\n",
    "def find_multiple(n: int, k: int) -> int:\n",
    "    if n % k == 0:\n",
    "        return n\n",
    "    return n + k - (n % k)\n",
    "\n",
    "MaskCache = torch.Tensor\n",
    "RoPECache = torch.Tensor\n",
    "KVCache = Tuple[torch.Tensor, torch.Tensor]\n",
    "llama_configs = {\n",
    "    \"0B\": dict(n_layer=2, n_head=4, n_embd=128),\n",
    "    \"7B\": dict(n_layer=32, n_head=32, n_embd=4096),\n",
    "    \"13B\": dict(n_layer=40, n_head=40, n_embd=5120),\n",
    "    \"30B\": dict(n_layer=60, n_head=52, n_embd=6656),\n",
    "    \"65B\": dict(n_layer=80, n_head=64, n_embd=8192),\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 32000\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embd: int = 4096\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.padded_vocab_size is None:\n",
    "            self.padded_vocab_size = find_multiple(self.vocab_size, 64)\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(**llama_configs[name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型结构\n",
    "`LLaMA`模型基于`Transformer`模型`Decoder`部分，做出了如下改进：\n",
    "- 前置归一化(Pre-Normalization)，并使用`RMSNorm`作为归一化函数\n",
    "- 使用`SwiGLU`作为激活函数\n",
    "- 使用旋转位置编码`RoPE`\n",
    "\n",
    "模型结构如下图所示：\n",
    "<p align=\"center\">\n",
    "    <img src=\"./_img/llama_arch.png\" width=\"65%\"/>\n",
    "</p>\n",
    "\n",
    "接下来对这三个部分做详细介绍：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Normalization\n",
    "Pre-Normalization是前置归一化技术。原始`Transformer`中的归一化方法属于`Post-Normalization`，`Pre-Normalization`就是讲归一化方法应用在每个子层之前（注意力层和MLP层），计算公式如下：\n",
    "$$Pre-Norm(x) = x + Sublayer(Norm(x))$$\n",
    "不同`Normalization`方法在模型中的位置对比如下图所示：\n",
    "<p align=\"center\">\n",
    "    <img src=\"./_img/diff_norm.png\" width=\"40%\"/>\n",
    "不同归一化模块的位置对比\n",
    "</p>\n",
    "\n",
    "相较于 `Post-Normalization`，`Pre-Normalization` 直接把每个子层加在了归一化模块之后，仅仅对输入的表示进行了归一化，从而可以防止模型的**梯度爆炸**或者**梯度消失**现象。虽然使用了 `Pre-Normalization` 的模型在训练过程中更加稳定，但是性能却逊色于采用了`Post-Normalization`的模型。\n",
    "\n",
    "此外，`RMSNorm`与传统`LayerNorm`函数相比，`RMSNorm`在保持训练稳定性和提升模型收敛速度的同时，还大幅提高了计算效率。\n",
    "\n",
    "`RMSNorm`计算公式为：\n",
    "$$RMSNorm: y =\\frac {x} {\\sqrt{Mean(x^2)+\\epsilon}} *\\gamma $$\n",
    "\n",
    "$$ Mean(x^2)=\\frac{1}{N} \\sum_{i=1}^N x_i^2$$\n",
    "\n",
    "RMSNorm 之所以能更高效，是因为其创造者发现 LayerNorm 的优势在于 rescaling invariance（译者注：指的是归一化过程能够适应输入数据的缩放，使得网络对这种缩放不敏感。），而非 recentering invariance（译者注：如果输入数据的均值发生了变化，但数据的分布形状和范围保持不变，那么具有 recentering invariance 的算法或函数的输出应该不受影响。）。基于这一发现，他们省略了归一化过程中的均值计算，使得算法更加简洁，而效果不减，且运算效率显著提升。\n",
    "<p align=\"center\">\n",
    "    <img src=\"./_img/LayerNorm_comp_RMSNorm.png\" width=\"80%\"/> <br>\n",
    "    层归一化（LayerNorm）与均方根归一化（RMSNorm）之间的方程差异\n",
    "</p>\n",
    "\n",
    "更多关于`RMSNorm`的解读见：[RMSNorm.ipynb](../../1-LLM-base-fundamentals/1.3-模型核心组件/归一化方法/RMSNorm.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "\n",
    "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
    "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        # x_normed = x / (rms_x + self.eps)\n",
    "        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
    "        return self.scale * x_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwiGLU 激活函数\n",
    "`LLaMA`在全连接层中使用了带有`SwiGLU`激活函数的`FFN`，SwiGLU 激活函数的公式为：\n",
    "$$Swish(x) = x \\cdot sigmoid(x)$$\n",
    "$$SwiGLU(x) = Swish(W^Gx) \\odot (W^Ux)$$\n",
    "\n",
    "这一改变旨在提升模型的性能。两者的核心差异在于：\n",
    "- ReLU 函数会将所有负数输入直接归零，而正数输入则保持不变。\n",
    "- 相比之下，**SwiGLU 函数含有一个可学习的参数 β，能够调节函数的插值程度。** 随着 β 值的增大，SwiGLU 的行为将逐渐接近 ReLU。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./_img/SwiGLU.png\" width=\"57%\"/>\n",
    "    <img src=\"./_img/regeswish.png\" width=\"41.5%\"/>\n",
    "ReLU 与 SwiGLU 在不同 β 值下的行为对比，可以看到当 β 达到 100 时，两者的曲线趋于一致，以及不同激活函数对比图。\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embd\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        n_hidden = find_multiple(n_hidden, 256)\n",
    "\n",
    "        self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_proj = nn.Linear(n_hidden, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 旋转式位置编码（Rotary Positional Embeddings）\n",
    "在位置编码上，`LLaMA`使用旋转位置嵌入（Rotary Positional Embeddings，RoPE）来代替原有的绝对位置编码。RoPE 借助了复数的思想，出发点是通过绝对位置编码的方式实现相对位置编码。\n",
    "\n",
    "关于`RoPE`的详细解读见：[RoPE.ipynb](../..//1-LLM-base-fundamentals/1.3-模型核心组件/位置编码/RoPE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x: torch.Tensor, rope_cache: RoPECache) -> torch.Tensor:\n",
    "    # truncate to support variable sizes\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    # cast because the reference does\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return x_out2.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制模块\n",
    "`Llama`中同样采用的是掩码注意力机制，实现上与前面讲的`GPT`中的`CausalSelfAttention`差别不大，主要区别在于这里加入了`RoPECache`，`MaskCache`和`KVCache`技术来加速训练和推理过程。所以这里，看这段代码主要是研究`Cache`技术是如何运用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        head_size = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, head_size)\n",
    "        q = q.view(B, T, self.n_head, head_size)\n",
    "        v = v.view(B, T, self.n_head, head_size)\n",
    "\n",
    "        q = apply_rope(q, rope)\n",
    "        k = apply_rope(k, rope)\n",
    "\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            cache_k, cache_v = kv_cache\n",
    "            # check if reached token limit\n",
    "            if input_pos[-1] >= max_seq_length:\n",
    "                input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n",
    "                # shift 1 position to the left\n",
    "                cache_k = torch.roll(cache_k, -1, dims=2)\n",
    "                cache_v = torch.roll(cache_v, -1, dims=2)\n",
    "            k = cache_k.index_copy(2, input_pos, k)\n",
    "            v = cache_v.index_copy(2, input_pos, v)\n",
    "            kv_cache = k, v\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        #  att = att.masked_fill(mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        #  att = F.softmax(att, dim=-1)\n",
    "        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y, kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    ") -> RoPECache:\n",
    "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "    transformers/rope/__init__.py. MIT License:\n",
    "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "    \"\"\"\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block\n",
    "下面是`Llama`中一个模块的构建代码，也就是一个`DecoderLayer`层的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.rms_2 = RMSNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        h, new_kv_cache = self.attn(self.rms_1(x), rope, mask, max_seq_length, input_pos, kv_cache)\n",
    "        x = x + h\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x, new_kv_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA\n",
    "下面是`LLaMA`模型的实现代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n",
    "                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "                ln_f=RMSNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.rope_cache: Optional[RoPECache] = None\n",
    "        self.mask_cache: Optional[MaskCache] = None\n",
    "        self.kv_caches: List[KVCache] = []\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.Tensor, max_seq_length: Optional[int] = None, input_pos: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[KVCache]]]:\n",
    "        B, T = idx.size()\n",
    "\n",
    "        block_size = self.config.block_size\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = block_size\n",
    "        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n",
    "        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n",
    "        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n",
    "\n",
    "        if self.rope_cache is None:\n",
    "            self.rope_cache = self.build_rope_cache(idx)\n",
    "        if self.mask_cache is None:\n",
    "            self.mask_cache = self.build_mask_cache(idx)\n",
    "\n",
    "        if input_pos is not None:\n",
    "            rope = self.rope_cache.index_select(0, input_pos)\n",
    "            mask = self.mask_cache.index_select(2, input_pos)\n",
    "            mask = mask[:, :, :, :max_seq_length]\n",
    "        else:\n",
    "            rope = self.rope_cache[:T]\n",
    "            mask = self.mask_cache[:, :, :T, :T]\n",
    "\n",
    "        # forward the model itself\n",
    "        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "        if input_pos is None:  # proxy for use_cache=False\n",
    "            for block in self.transformer.h:\n",
    "                x, _ = block(x, rope, mask, max_seq_length)\n",
    "        else:\n",
    "            if not self.kv_caches:\n",
    "                head_size = self.config.n_embd // self.config.n_head\n",
    "                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n",
    "                self.kv_caches = [\n",
    "                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n",
    "                    for _ in range(self.config.n_layer)\n",
    "                ]\n",
    "            for i, block in enumerate(self.transformer.h):\n",
    "                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(LLaMAConfig.from_name(name))\n",
    "\n",
    "    def build_rope_cache(self, idx: torch.Tensor) -> RoPECache:\n",
    "        return build_rope_cache(\n",
    "            seq_len=self.config.block_size,\n",
    "            n_elem=self.config.n_embd // self.config.n_head,\n",
    "            dtype=idx.dtype,\n",
    "            device=idx.device,\n",
    "        )\n",
    "\n",
    "    def build_mask_cache(self, idx: torch.Tensor) -> MaskCache:\n",
    "        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n",
    "        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def reset_cache(self) -> None:\n",
    "        self.kv_caches.clear()\n",
    "        if self.mask_cache.device.type == \"xla\":\n",
    "            # https://github.com/Lightning-AI/lit-parrot/pull/83#issuecomment-1558150179\n",
    "            self.rope_cache = None\n",
    "            self.mask_cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行测试\n",
    "下面先简单运行一下`LLaMA`模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=1024\n",
    "vocab_size=32000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型配置，由于`7B`及以上的模型层数较多，初始化和推理的速度都比较慢，这里自定义一个`0B`模型，只有2层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7B config LLaMAConfig(block_size=1024, vocab_size=32000, padded_vocab_size=32000, n_layer=2, n_head=4, n_embd=128)\n"
     ]
    }
   ],
   "source": [
    "config = LLaMAConfig.from_name(\"0B\") # n_layer=32, n_head=32, n_embd=4096\n",
    "config.block_size = block_size\n",
    "print(\"7B config\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA(\n",
      "  (lm_head): Linear(in_features=128, out_features=32000, bias=False)\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(32000, 128)\n",
      "    (h): ModuleList(\n",
      "      (0-1): 2 x Block(\n",
      "        (rms_1): RMSNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=128, out_features=384, bias=False)\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (rms_2): RMSNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc1): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_fc2): Linear(in_features=128, out_features=512, bias=False)\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): RMSNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LLaMA(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试数据\n",
    "随机生成一个测试数据，`input`是tokenize之后的向量，每个值代表`token id`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n",
      "torch.Size([16, 1024])\n",
      "batch size:16, token length1024 \n"
     ]
    }
   ],
   "source": [
    "input = torch.randint(0,vocab_size,(16,1024))\n",
    "target = torch.randint(0,vocab_size,(16,1024))\n",
    "\n",
    "print(input.shape)\n",
    "print(target.shape)\n",
    "print(\"batch size:{}, token length{} \".format(input.shape[0],input.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行一次前向推理，并计算loss损失。结果为词表的概率分布，与[GPT](../../2-主流模型架构/2.1-GPT系列/nanoGPT-run.ipynb)中所讲的相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape:torch.Size([16, 1024, 32000])----Vocab size: 32000\n",
      "Loss: 10.540298461914062\n"
     ]
    }
   ],
   "source": [
    "logits = model(input)\n",
    "print(f\"logits.shape:{logits.shape}----Vocab size: {config.vocab_size}\")\n",
    "loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1), ignore_index=-1)\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward详解\n",
    "下面逐行详细讲一下`forward`函数的前向推理过程。\n",
    "\n",
    "输入参数：\n",
    "- `idx: torch.Tensor`，输入的token张量，维度为`[batch_size, token_length]\n",
    "- `max_seq_length: Optional[int] = None`：可选的最大序列长度。如果未提供，则使用配置中的 `block_size`作为最大序列长度。\n",
    "- `input_pos: Optional[torch.Tensor] = None`：可选的位置索引张量。如果提供，形状应为 (token_length,)，用于指定输入序列中每个位置的具体位置索引。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = input\n",
    "max_seq_length = None\n",
    "input_pos = None\n",
    "\n",
    "# 重置一下模型，清空cache\n",
    "model = LLaMA(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先输入的`batch_size=166`和`token_length=1024`。首先会判断输入的token长度是否小于`max_seq_length`。满足关系`T<=max_seq_length<=block_size`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:16, token length1024 \n"
     ]
    }
   ],
   "source": [
    "B, T = idx.size()\n",
    "print(\"batch size:{}, token length{} \".format(B,T))\n",
    "\n",
    "block_size = model.config.block_size\n",
    "if max_seq_length is None:\n",
    "    max_seq_length = block_size\n",
    "assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n",
    "assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n",
    "assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建RoPEcache和Maskcache\n",
    "\n",
    "下面是对`build_rope_cache`的逐行解读。\n",
    "输入参数：\n",
    "- seq_len (int): 序列长度。\n",
    "- n_elem (int): 每个位置的元素数量。\n",
    "- dtype (torch.dtype): 数据类型。\n",
    "- device (torch.device): 设备（CPU 或 GPU）。\n",
    "- base (int): 基数，用于计算频率的基数，默认值为 10000。\n",
    "\n",
    "`theta`计算公式：\n",
    "$\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    ") -> RoPECache:\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    # 计算频率 theta。torch.arange(0, n_elem, 2, dtype=dtype, device=device) 生成从 0 到 n_elem 的步长为 2 的序列，然后除以 n_elem，再用 base 的这些值的幂的倒数来计算频率。\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    # 创建位置索引 seq_idx，生成从 0 到 seq_len 的序列。\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    # 计算旋转位置编码 idx_theta，通过 seq_idx 和 theta 的外积计算得到。\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    # 计算正弦和余弦编码，并将它们沿最后一个维度拼接，得到最终的旋转位置编码缓存 cache。\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_mask_cache`中代码解读见: \n",
    "- [torch.ones](../../1-LLM-base-fundamentals/1.6-Pytorch常用函数/生成矩阵.ipynb)，\n",
    "- [torch.tril](../../1-LLM-base-fundamentals/1.6-Pytorch常用函数/矩阵操作.ipynb)，\n",
    "- [torch.unsqueeze](../../1-LLM-base-fundamentals/1.6-Pytorch常用函数/矩阵操作.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ropes_cache before init: None\n",
      "torch.Size([1, 1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 此时模型的rope_cache和mask_cache都为空\n",
    "print(f\"ropes_cache before init: {model.rope_cache}\")\n",
    "if model.rope_cache is None:\n",
    "    model.rope_cache = model.build_rope_cache(idx)\n",
    "if model.mask_cache is None:\n",
    "    model.mask_cache = model.build_mask_cache(idx)\n",
    "    print(model.mask_cache.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_pos\n",
    "> `input_pos` 是一个可选的 `torch.Tensor`，用于指定输入序列中每个位置的具体位置索引。它的作用是允许模型在处理输入序列时使用特定的位置编码，而不是默认的顺序位置编码。\n",
    "> 在代码中，如果 `input_pos` 不为 `None`，则会使用 `input_pos` 来从 `rope_cache` 和 `mask_cache` 中选择相应的位置编码和掩码。这意味着模型可以根据 `input_pos` 提供的索引来处理输入序列，而不是假设输入序列的位置是连续的。\n",
    "> 具体来说：\n",
    "> - `rope = self.rope_cache.index_select(0, input_pos)`：从 `rope_cache` 中选择 input_pos 指定的位置编码。\n",
    "> - `mask = self.mask_cache.index_select(2, input_pos)`：从 `mask_cache` 中选择 `input_pos` 指定的掩码。\n",
    "> 这样做的好处是可以灵活地处理输入序列的位置编码，适应不同的输入需求。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_pos is not None:\n",
    "    rope = model.rope_cache.index_select(0, input_pos)\n",
    "    mask = model.mask_cache.index_select(2, input_pos)\n",
    "    mask = mask[:, :, :, :max_seq_length]\n",
    "else:\n",
    "    rope = model.rope_cache[:T]\n",
    "    mask = model.mask_cache[:, :, :T, :T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings\n",
    "embedding之后的维度为`[batch_size, token_length, n_embedding]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dim: 128\n",
      "shape before embedding :torch.Size([16, 1024]) shape after embedding: torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "x = model.transformer.wte(idx)\n",
    "print(f\"embedding dim: {config.n_embd}\")\n",
    "print(f\"shape before embedding :{idx.shape} shape after embedding: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention & MLP 计算\n",
    "接下来是`forward`过程中的核心部分，`Attention`注意力计算和`MLP`计算，这两个计算是每一层都会计算几次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block layers: 2\n",
      "shape after block:  torch.Size([16, 1024, 128])\n",
      "shape after block:  torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "print(f\"block layers: {len(model.transformer.h)}\")\n",
    "if input_pos is None:  # proxy for use_cache=False\n",
    "    for block in model.transformer.h:\n",
    "        x, _ = block(x, rope, mask, max_seq_length)\n",
    "        print(\"shape after block: \", x.shape)\n",
    "else:\n",
    "    if not model.kv_caches:\n",
    "        head_size = model.config.n_embd // model.config.n_head\n",
    "        cache_shape = (B, model.config.n_head, max_seq_length, head_size)\n",
    "        model.kv_caches = [\n",
    "            (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n",
    "            for _ in range(model.config.n_layer)\n",
    "        ]\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        x, model.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, model.kv_caches[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最后的FFN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after ln_f shape: torch.Size([16, 1024, 128])\n"
     ]
    }
   ],
   "source": [
    "x = model.transformer.ln_f(x)\n",
    "print(f\"after ln_f shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词表概率映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output logits  torch.Size([16, 1024, 32000])\n"
     ]
    }
   ],
   "source": [
    "logits = model.lm_head(x)\n",
    "print(\"output logits \",logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KVCache\n",
    "关于KVCache的详细解读见：\n",
    "[9-推理加速/KVCache/KVCache.ipynb](../../9-推理加速/KVCache/KVCache.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
