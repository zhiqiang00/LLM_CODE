{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719cbb51",
   "metadata": {},
   "source": [
    "# Padding & Mask\n",
    "这里，我们单独讲讲在Encoder和Decoder中的Padding和Mask的作用。这两个在所有LLM中都是必不可少的。简单来讲：\n",
    "- padding和mask都是输入序列中的一种特殊token，不参与注意力的计算\n",
    "- Padding是为了保证输入序列具有相同的长度，我们需要对序列添加padding使序列长度相同\n",
    "- 在预训练过程中，对需要预测的token作mask，在微调中，Decoder中，计算到第$i$个token时，对其之后的token作mask，使注意力机制只关注第$i$个token之前。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c59e5",
   "metadata": {},
   "source": [
    "## 什么是Padding？\n",
    "Padding是一个与输入序列形状相同的矩阵，用于指示哪些位置的token是需要参与注意力计算，哪些位置的token则不需要计算，是填充值。\n",
    "- Padding矩阵中，`0`表示真实值，是需要计算注意力的。\n",
    "- `1`表示填充值，不需要计算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031d055",
   "metadata": {},
   "source": [
    "## 如何使用Padding\n",
    "在`Self-Attention`的计算过程中，我们计算`Query`查询和`Key`键的`Dot-product`点积来得到注意力分数。在应用softmax函数之前，我们可以使用`padding`来确保填充位置的注意力分数为一个非常大的负数（例如，乘以-1e9）。这样，当应用softmax函数时，这些位置的权重将接近于零，从而确保模型在其计算中忽略这些填充值。\n",
    "\n",
    "如下面这个示例：\n",
    "假设我们有一个长度为4的序列：`[A, B, C, <pad>]`，其中`<pad>`是填充标记。对应的padding mask是：`[0, 0, 0, 1]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09c072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "attention_scores = torch.tensor([[3, 4, 192, 1],[2, 8, 1,1]]) \n",
    "mask = torch.tensor([[0,0,0,1],[0,0, 1,1]])\n",
    "attention_scores = attention_scores.masked_fill(mask == 1, -1e9)\n",
    "print(attention_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae502750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[          3,           4,         192, -1000000000],\n",
      "        [          2,           8, -1000000000, -1000000000]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9304fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cd110aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: torch.Size([2, 4])\n",
      "trg: torch.Size([2, 3])\n",
      "src batch max length: 3\n",
      "trg batch max length: 4\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "k_pad_idx = 0\n",
    "q_pad_idx = 0\n",
    "\n",
    "# token序列 \n",
    "# src: 第一句话长度为3，第二句话长度为4， \n",
    "# 在这个batch中，batch_length=4，第一句话需要padding一个0 \n",
    "src_token = torch.tensor([[3, 4, 192, 0],[2, 8, 5, 3]]) \n",
    "\n",
    "# trg: 第一句话长度为2，第二句话长度为3\n",
    "# 在这个batch中，batch_length=3，第一句话需要padding一个0 \n",
    "trg_token = torch.tensor([[6, 7, 0],[11, 28, 9]])\n",
    "\n",
    "print(\"src:\", src_token.shape)\n",
    "print(\"trg:\", trg_token.shape)\n",
    "\n",
    "len_q, len_k = trg_token.size(1), src_token.size(1)\n",
    "print(\"src batch max length:\", len_q)\n",
    "print(\"trg batch max length:\", len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeding\n",
    "# src = torch.randn(2, 4, 512) # 2个batch， 4个长度， 512维度\n",
    "# trg = torch.randn(2, 3, 512) # 2个batch， 3个长度， 512维度\n",
    "\n",
    "# 多头Q\n",
    "# src encode k : 2个batch， 8个头， 4个长度， 单头64维度\n",
    "src = torch.randn(2, 8, 4, 64) \n",
    "\n",
    "# trg decode Q : 2个batch， 8个头， 3个长度， 单头64维度\n",
    "trg = torch.randn(2, 8, 3, 64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb08196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token:\n",
      " tensor([[  3,   4, 192,   0],\n",
      "        [  2,   8,   5,   3]])\n",
      "trg_token:\n",
      " tensor([[8, 2, 0],\n",
      "        [5, 2, 2]])\n",
      "--------------------------------------------------------\n",
      "src_mask:\n",
      " tensor([[ True,  True,  True, False],\n",
      "        [ True,  True,  True,  True]])\n",
      "trg_mask:\n",
      " tensor([[ True,  True, False],\n",
      "        [ True,  True,  True]])\n",
      "--------------------------------------------------------\n",
      "【src_token】:torch.Size([2, 4])->【src_mask】: torch.Size([2, 1, 1, 4])\n",
      "【trg_token】:torch.Size([2, 3])->【trg_mask】: torch.Size([2, 1, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 为配合多头注意力，需要填充维度\n",
    "src_mask = src_token.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "trg_mask = trg_token.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "print(\"src_token:\\n\", src_token)\n",
    "print(\"trg_token:\\n\", trg_token)\n",
    "print('--------------------------------------------------------')\n",
    "print(\"src_mask:\\n\", src_token.ne(k_pad_idx))\n",
    "print(\"trg_mask:\\n\", trg_token.ne(q_pad_idx))\n",
    "print('--------------------------------------------------------')\n",
    "print(f\"【src_token】:{src_token.shape}->【src_mask】:\",src_mask.shape)\n",
    "print(f\"【trg_token】:{trg_token.shape}->【trg_mask】:\",trg_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ce615c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【src_mask】:torch.Size([2, 1, 1, 4]) -> 【src_mask_repeat】:torch.Size([2, 1, 3, 4]) \n",
      "【trg_mask】:torch.Size([2, 1, 3, 1]) -> 【trg_mask_repeat】:torch.Size([2, 1, 3, 4]) \n"
     ]
    }
   ],
   "source": [
    "# 批量处理\n",
    "src_mask_repeat = src_mask.repeat(1, 1, len_q, 1)\n",
    "trg_mask_repeat = trg_mask.repeat(1, 1, 1, len_k)\n",
    "print(f\"【src_mask】:{src_mask.shape} -> 【src_mask_repeat】:{src_mask_repeat.shape} \")\n",
    "print(f\"【trg_mask】:{trg_mask.shape} -> 【trg_mask_repeat】:{trg_mask_repeat.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ccef8d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "------------------------------\n",
      "src_mask \n",
      " tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [1, 1, 1, 0]], dtype=torch.int32)\n",
      "trg_mask \n",
      " tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n",
      "------------------------------\n",
      "mask \n",
      " tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# & and 操作符 1&1=1, 1&0=0, 0&1=0, 0&0=0\n",
    "mask = src_mask_repeat & trg_mask_repeat #[2,1,3,4] & [2,1,3,4] =[2,1,3,4]\n",
    "print(mask[0][0].shape)\n",
    "print('------------------------------')\n",
    "print(\"src_mask \\n\",src_mask_repeat[0][0].int())\n",
    "print(\"trg_mask \\n\", trg_mask_repeat[0][0].int())\n",
    "print('------------------------------')\n",
    "print(\"mask \\n\",mask[0][0].int())\n",
    "# token序列，q=[6, 7, 0], k=[3, 4, 192, 0]， 0为padding-index\n",
    "# 第一排 1，1，1，0 对应 6 与 3， 4， 192， 0之间的attention mask\n",
    "# 第二排 1，1，1，0 对应 7 与 3， 4， 192， 0之间的attention mask\n",
    "# 第三排 0，0，0，0 对应 0 与 3， 4， 192， 0之间的attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2f72bb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Decode Self Masked \n",
    "# mask_decode = \n",
    "mask_decode = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor)\n",
    "print(mask_decode.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "000c8fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 64])\n",
      "tensor([[  1.1102,  -3.2859, -12.9273,     -inf],\n",
      "        [ -0.7410,  -2.2688,  14.2815,     -inf],\n",
      "        [    -inf,     -inf,     -inf,     -inf]])\n"
     ]
    }
   ],
   "source": [
    "# 计算 encode K， decode Q之间的注意力分数\n",
    "q = trg\n",
    "k_t = src.transpose(2,3)\n",
    "\n",
    "print(xq.shape)\n",
    "score = xq @ xk_t\n",
    "\n",
    "score_mask = score.masked_fill(mask == 0, -torch.inf)\n",
    "# score_mask = score.masked_fill(mask == 0, -10000)\n",
    "# print(score_mask)\n",
    "print(score_mask[0,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee915d4a-1855-4a1e-af03-c79aa2b40457",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq 10,   6\n",
    "\n",
    "mask=zeros(10,10)\n",
    "mask[:6,:6] = ones.tril(6,6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
