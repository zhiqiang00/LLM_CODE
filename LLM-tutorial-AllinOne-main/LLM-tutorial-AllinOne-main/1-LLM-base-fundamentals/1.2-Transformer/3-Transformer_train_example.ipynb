{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在正式开始训练之前，我们先用一个简单的示例跑一跑训练过程。下面的示例是一个复制内容的任务，给定一个词汇表中的随机字符串，目标是生成相同的字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合成数据\n",
    "下面是一个产生批次数据的函数，传入词表`V`，批次大小`batch_size`，批次数量`nbatched`，然后一个批次一个批次的生成数据，这里建议好好看了一下`yield`函数，这个函数不会一次返回所有批次的数据，而是一个批次一个批次的返回，只有当模型需要数据的时候，才会返回一个批次数据，这对于大规模预训练中，是必不可少的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "返回一个张量，其中充满在low（包含）和high（不包含）之间均匀生成的随机整数。 张量的形状由参数siz定义。\n",
    "\n",
    "Parameters\n",
    "\n",
    "-  **low**: 生成的随机整数的下界. Default: 0.\n",
    "-  **high**： 生成的随机整数的上界.\n",
    "-  **size**： 生成的随机整数的张量的形状，可以是一个整数或元组\n",
    "    \n",
    "Keyword Arguments\n",
    "-   **generator**：一个用于采样的伪随机数生成器\n",
    "-   **out**： 输出的tensor.\n",
    "-   **dtype** (torch.dtype, optional)： 生成的数据类型，如果为None，是 `torch.int64`.\n",
    "-   **layout**：返回张量的期望布局. Default: `torch.strided`.\n",
    "-   **device**：返回张量的期望装置。Default：如果为None，则使用当前设备作为默认张量类型。设备将是CPU张量类型的CPU和当前CUDA张量类型的CUDA设备。\n",
    "-   **requiresgrad**：If autograd should record operations on the returned tensor. Default: False.\n",
    "    \n",
    "\n",
    ">>> torch.randint(3, 10, (2, 2))\n",
    "tensor([[4, 5],\n",
    "        [6, 7]])\n",
    "\n",
    "\"\"\"\n",
    "torch.randint(low=0, high, size, \\*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requiresgrad=False) → [Tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detach的作用\n",
    "detach()函数在Pytorch中用于从当前计算图中分离张量，分离是创建一个新的张量，原始张量在计算图中依旧存在，并不受影响。但是新张量不参与梯度计算，与原始张量共享数据。因此，对分离后的张量进行的任何操作都不会影响原始张量，也不会在计算图中留下任何痕迹。\n",
    "\n",
    "由于涉及到计算图，这个会在后续进行补充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.batch import Batch\n",
    "\n",
    "def data_gen(V, batchsize, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batchsize, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach() # detach()返回一个新的Variable，从当前计算图中分离下来的，但是仍指向原变量的存放位置，即如果原变量的数据发生了改变，新的Variable\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss计算\n",
    "下面代码中的generator就是在前文中提到的Genrator类，用以在Decoder之后的线性变化和Softmax操作，最终输出词的预测概率。这里是把这个操作放到了loss计算中，当然也可以先计算再传入到loss计算里面。criterion就是损失计算函数，用的是前文中写的LabelSmoothing。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@QA\n",
    "@Q 讲一下contiguous()函数的作用\n",
    "contiguous()就相当于深拷贝操作，拷贝一份tensor，保证接下来的操作不会对原tensor造成影响。这里要注意的是在torch的，对tensor进行narrow()、view()、expand()和transpose()等操作都不会创建新的tensor，都是在原数据的基础上进行操作，也就是操作前后的tensor是共享内存的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@QA\n",
    "@Q 讲一下view()函数的作用\n",
    "按照传入的参数变换维度，与转置不同，view操作会先按行将tensor展开成一维，然后按照传入参数的维度要求，去组成对应维度的tensor。看下面这个示例：\n",
    "import torch\n",
    "a=torch.Tensor([[[1,2,3],[4,5,6]]]) # torch.Size([1, 2, 3])\n",
    "print(a.view(3,2))                  # torch.Size([3, 2])\n",
    "输出：\n",
    "tensor([[1., 2.],\n",
    "        [3., 4.],\n",
    "        [5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贪心解码\n",
    "贪心解码策略就是每步都选取概率最高的词作为预测的词。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import subsequent_mask\n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练\n",
    "下面开始训练，词表大小为11，EncodeLayer和DecoderLayer只有2层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.12 | Tokens / Sec:  2189.2 | Learning Rate: 5.5e-06\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.06 | Tokens / Sec:  2487.9 | Learning Rate: 6.1e-05\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.83 | Tokens / Sec:  2549.1 | Learning Rate: 1.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.48 | Tokens / Sec:  2891.8 | Learning Rate: 1.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.11 | Tokens / Sec:  2678.7 | Learning Rate: 2.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.66 | Tokens / Sec:  2765.7 | Learning Rate: 2.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.34 | Tokens / Sec:  2566.6 | Learning Rate: 3.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.17 | Tokens / Sec:  2775.3 | Learning Rate: 3.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.14 | Tokens / Sec:  2683.9 | Learning Rate: 4.5e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.18 | Tokens / Sec:  2658.5 | Learning Rate: 5.0e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.14 | Tokens / Sec:  2618.5 | Learning Rate: 5.6e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.16 | Tokens / Sec:  2754.3 | Learning Rate: 6.1e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.09 | Tokens / Sec:  2775.8 | Learning Rate: 6.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.14 | Tokens / Sec:  2510.1 | Learning Rate: 7.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  2726.9 | Learning Rate: 7.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec:  2568.2 | Learning Rate: 8.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.05 | Tokens / Sec:  2748.1 | Learning Rate: 8.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  2736.3 | Learning Rate: 9.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.10 | Tokens / Sec:  2847.1 | Learning Rate: 1.0e-03\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.16 | Tokens / Sec:  2677.3 | Learning Rate: 1.1e-03\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "from conf.settings import DummyOptimizer, DummyScheduler\n",
    "from Train import LabelSmoothing\n",
    "from Train.learning_rate import rate\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from Train.train import run_epoch\n",
    "from EncoderDecoder import make_model\n",
    "\n",
    "def example_simple_model():\n",
    "    V = 11\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "    model = make_model(V, V, N=2)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    batch_size = 80\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 20),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        model.eval()\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 5),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )[0]\n",
    "\n",
    "    model.eval()\n",
    "    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "    max_len = src.shape[1]\n",
    "    src_mask = torch.ones(1, 1, max_len)\n",
    "    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0))\n",
    "\n",
    "\n",
    "example_simple_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"imgs/train/train_example_res.png\" width=\"100%\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
