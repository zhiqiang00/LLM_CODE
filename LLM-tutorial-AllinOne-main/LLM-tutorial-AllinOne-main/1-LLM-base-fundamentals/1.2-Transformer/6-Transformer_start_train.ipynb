{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来真真真真的要开始训练**Transformer**模型了。\n",
    "\n",
    "为了更好的理解接下来要讲的训练过程，下面会使用到大量在前面几篇文章中使用到的代码，部分会重新贴出，部分则直接引用，需要clone代码仓库中的代码才能够运行，所以，建议clone完GitHub代码后，直接看对应的notebook进行学习。\n",
    "\n",
    "其实，当模型设计好，数据处理好，让模型训练起来这一步，反而是最简单的，下面的代码基本上也很容易看懂，无非就是把前面讲过的全部串联起来，开始训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导包 / Import packages\n",
    "首先是导包，并加载tokenizer和词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/transformer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "8317\n",
      "6384\n"
     ]
    }
   ],
   "source": [
    "from EncoderDecoder import make_model\n",
    "from utils.LabelSmoothing import LabelSmoothing\n",
    "from DataLoader import create_dataloaders\n",
    "from Train.learning_rate import rate\n",
    "from Train.train import TrainState\n",
    "from utils.batch import Batch\n",
    "from Train.SimpleLossCompute import SimpleLossCompute\n",
    "from DataLoader import load_tokenizers, load_vocab\n",
    "from conf.settings import DummyOptimizer, DummyScheduler\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import time, os\n",
    "import GPUtil\n",
    "spacy_de, spacy_en = load_tokenizers()\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de, spacy_en) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练一个epoch / Run one epoch\n",
    "下面是训练一个epoch的代码，一个epoch的训练过程包括：**前向传播***，**计算Loss**，**反向传播**，**更新参数**，**更新学习率**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练准备\n",
    "训练任何模型都离不开以下这几个部分：**模型**，**分布式训练**，**损失函数**，**数据迭代器加载**，**学习率初始化**，**训练一个epoch**，**保存模型**，**模型评估**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    ngpus_per_node,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    config,\n",
    "    is_distributed=False,\n",
    "):\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "    # 初始化模型\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6, d_model=d_model)\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "    \n",
    "    # 初始化分布式训练\n",
    "    if is_distributed:\n",
    "        dist.init_process_group( # 初始化进程组\n",
    "            \"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node\n",
    "        )\n",
    "        model = DDP(model, device_ids=[gpu]) # 初始化分布式数据并行\n",
    "        module = model.module\n",
    "        is_main_process = gpu == 0\n",
    "\n",
    "    # 初始化损失函数\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "\n",
    "    # 初始化数据加载器\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "        max_padding=config[\"max_padding\"],\n",
    "        is_distributed=is_distributed,\n",
    "    )\n",
    "\n",
    "    # 初始化优化器和学习率调度器\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    # 初始化学习率调度器\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "\n",
    "    # 开始训练\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        if is_distributed:\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "            valid_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train() # 设置模型为训练模式\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        \n",
    "        # 训练一个epoch\n",
    "        _, train_state = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization() # 显示GPU使用情况\n",
    "        if is_main_process: # 是否为主进程\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path) # 保存模型\n",
    "        torch.cuda.empty_cache() # 清空GPU缓存\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval() # 设置模型为评估模式\n",
    "        sloss = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train worker process using GPU: 0 for training\n",
      "[GPU0] Epoch 0 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   7.66 | Tokens / Sec:   673.7 | Learning Rate: 5.4e-07\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   7.44 | Tokens / Sec:  6590.7 | Learning Rate: 1.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   7.00 | Tokens / Sec:  6369.6 | Learning Rate: 2.2e-05\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   6.70 | Tokens / Sec:  6510.9 | Learning Rate: 3.3e-05\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   6.43 | Tokens / Sec:  6089.0 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   6.39 | Tokens / Sec:  6147.8 | Learning Rate: 5.4e-05\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   6.21 | Tokens / Sec:  6111.0 | Learning Rate: 6.5e-05\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   5.97 | Tokens / Sec:  6329.0 | Learning Rate: 7.6e-05\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   5.76 | Tokens / Sec:  6204.4 | Learning Rate: 8.7e-05\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   5.65 | Tokens / Sec:  6076.2 | Learning Rate: 9.7e-05\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   5.37 | Tokens / Sec:  6182.9 | Learning Rate: 1.1e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   5.06 | Tokens / Sec:  6116.8 | Learning Rate: 1.2e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   4.95 | Tokens / Sec:  6205.7 | Learning Rate: 1.3e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   4.67 | Tokens / Sec:  6104.7 | Learning Rate: 1.4e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   4.65 | Tokens / Sec:  6093.5 | Learning Rate: 1.5e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   4.33 | Tokens / Sec:  6280.9 | Learning Rate: 1.6e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   4.38 | Tokens / Sec:  7691.8 | Learning Rate: 1.7e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   4.47 | Tokens / Sec:  7331.2 | Learning Rate: 1.8e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   4.24 | Tokens / Sec:  6757.5 | Learning Rate: 1.9e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   4.11 | Tokens / Sec:  6533.6 | Learning Rate: 2.0e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   3.98 | Tokens / Sec:  6294.5 | Learning Rate: 2.2e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   3.98 | Tokens / Sec:  5952.3 | Learning Rate: 2.3e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   3.86 | Tokens / Sec:  5983.5 | Learning Rate: 2.4e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  6% |\n",
      "|  1 |  0% |  4% |\n",
      "|  2 |  0% |  0% |\n",
      "|  3 |  0% |  0% |\n",
      "[GPU0] Epoch 0 Validation ====\n",
      "(tensor(3.8984, device='cuda:0'), <Train.train.TrainState object at 0x7f714c2bc400>)\n",
      "[GPU0] Epoch 1 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   4.13 | Tokens / Sec:  4442.8 | Learning Rate: 2.4e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   3.68 | Tokens / Sec:  6497.6 | Learning Rate: 2.6e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   3.78 | Tokens / Sec:  6448.3 | Learning Rate: 2.7e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   3.75 | Tokens / Sec:  6372.8 | Learning Rate: 2.8e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   3.68 | Tokens / Sec:  6387.4 | Learning Rate: 2.9e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   3.61 | Tokens / Sec:  6201.2 | Learning Rate: 3.0e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   3.44 | Tokens / Sec:  6303.8 | Learning Rate: 3.1e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   3.44 | Tokens / Sec:  6305.6 | Learning Rate: 3.2e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   3.50 | Tokens / Sec:  6308.6 | Learning Rate: 3.3e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   3.60 | Tokens / Sec:  6307.3 | Learning Rate: 3.4e-04\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   3.35 | Tokens / Sec:  6628.7 | Learning Rate: 3.5e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   3.17 | Tokens / Sec:  6401.7 | Learning Rate: 3.6e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   3.35 | Tokens / Sec:  6398.5 | Learning Rate: 3.7e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   3.43 | Tokens / Sec:  6365.7 | Learning Rate: 3.8e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   3.22 | Tokens / Sec:  6224.1 | Learning Rate: 4.0e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   3.41 | Tokens / Sec:  6173.4 | Learning Rate: 4.1e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   3.11 | Tokens / Sec:  6508.7 | Learning Rate: 4.2e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   3.02 | Tokens / Sec:  6321.8 | Learning Rate: 4.3e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   3.19 | Tokens / Sec:  6665.4 | Learning Rate: 4.4e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   3.36 | Tokens / Sec:  6241.0 | Learning Rate: 4.5e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   3.11 | Tokens / Sec:  6278.9 | Learning Rate: 4.6e-04\n",
      "Epoch Step:    841 | Accumulation Step:  85 | Loss:   3.07 | Tokens / Sec:  6275.5 | Learning Rate: 4.7e-04\n",
      "Epoch Step:    881 | Accumulation Step:  89 | Loss:   2.81 | Tokens / Sec:  6056.3 | Learning Rate: 4.8e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% |  7% |\n",
      "|  1 |  0% |  4% |\n",
      "|  2 |  0% |  0% |\n",
      "|  3 |  0% |  0% |\n",
      "[GPU0] Epoch 1 Validation ====\n",
      "(tensor(2.8503, device='cuda:0'), <Train.train.TrainState object at 0x7f714c2bc400>)\n",
      "[GPU0] Epoch 2 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   2.69 | Tokens / Sec:  5030.0 | Learning Rate: 4.9e-04\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   2.88 | Tokens / Sec:  6513.1 | Learning Rate: 5.0e-04\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   2.70 | Tokens / Sec:  6520.3 | Learning Rate: 5.1e-04\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   2.69 | Tokens / Sec:  6454.5 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   2.73 | Tokens / Sec:  6381.8 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   2.52 | Tokens / Sec:  6449.2 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   2.63 | Tokens / Sec:  6330.9 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   2.78 | Tokens / Sec:  6364.7 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   2.78 | Tokens / Sec:  6498.3 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   2.42 | Tokens / Sec:  6502.1 | Learning Rate: 5.9e-04\n"
     ]
    }
   ],
   "source": [
    "def train_distributed_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "\n",
    "    ngpus = torch.cuda.device_count()\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12356\"\n",
    "    print(f\"Number of GPUs detected: {ngpus}\")\n",
    "    print(\"Spawning training processes ...\")\n",
    "    mp.spawn(\n",
    "        train_worker,\n",
    "        nprocs=ngpus,\n",
    "        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config):\n",
    "    if config[\"distributed\"]:\n",
    "        train_distributed_model(\n",
    "            vocab_src, vocab_tgt, spacy_de, spacy_en, config\n",
    "        )\n",
    "    else:\n",
    "        train_worker(\n",
    "            0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n",
    "        )\n",
    "\n",
    "\n",
    "def load_trained_model():\n",
    "    config = {\n",
    "        \"batch_size\": 32,\n",
    "        \"distributed\": False,\n",
    "        \"num_epochs\": 8,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "    if not os.path.exists(model_path):\n",
    "        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n",
    "\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), N=6)\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一部分的学习建议多去对比不同模型训练代码上的差异，整体步骤其实相差不大，最大的差别其实还是在数据的预处理上。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
