{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们讲已经讲完了如何构建一个Transformer模型，在接下来的文章中，我们将继续讲解如何去训练模型。本文中，则首先讲解Transformer的训练机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批处理对象 / Batches and Masking\n",
    "在训练模型的时候，数据都是分批次的给到模型去训练，而不是一条一条或者一次全部给到模型；并且还需要对数据的padding部分和掩码部分做mask操作。因此，我们需要定义一个批处理对象，其中包含了用于训练的src和target数据，并对数据进行mask操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环主函数 / Training Loop\n",
    "下面创建训练一个Epoch的核心函数，遍历每个批次的数据，前向传播，计算损失，反向传播以及更新梯度。核心为这四步，不管什么模型都一样。剩下的就是记录日志和训练状态。下面给出的代码中TrainState就是记录训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # 在当前epoch中训练的步数\n",
    "    accum_step: int = 0  # 进行梯度累计的次数\n",
    "    samples: int = 0  # 使用的样本数量\n",
    "    tokens: int = 0  # 已经处理的token数量\n",
    "        \n",
    "def run_epoch(\n",
    "    data_iter, # 数据迭代器\n",
    "    model, # 模型\n",
    "    loss_compute, # 计算损失的函数\n",
    "    optimizer, # 优化器\n",
    "    scheduler, # 学习率调度器\n",
    "    mode=\"train\", # 模式\n",
    "    accum_iter=1, # 梯度累计的次数\n",
    "    train_state=TrainState(), # 训练状态\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0 # 记录总的token数量\n",
    "    total_loss = 0 # 记录总的损失\n",
    "    tokens = 0 # 记录当前epoch已经处理的token数量\n",
    "    n_accum = 0 # 记录当前epoch已经进行的梯度累计次数\n",
    "    for i, batch in enumerate(data_iter): # 遍历数据迭代器\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask # 前向传播\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens) # 计算损失\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward() # 反向传播\n",
    "            train_state.step += 1 # 更新训练步数\n",
    "            train_state.samples += batch.src.shape[0] # 更新样本数量\n",
    "            train_state.tokens += batch.ntokens # 更新token数量\n",
    "            if i % accum_iter == 0: # 每accum_iter步更新一次参数\n",
    "                optimizer.step() # 更新参数\n",
    "                optimizer.zero_grad(set_to_none=True) # 梯度清零\n",
    "                n_accum += 1 # 更新梯度累计次数\n",
    "                train_state.accum_step += 1 # 更新累计步数\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss # 更新总的损失\n",
    "        total_tokens += batch.ntokens # 更新总的token数量\n",
    "        tokens += batch.ntokens # 更新当前epoch\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"): # 每40步打印一次信息\n",
    "            lr = optimizer.param_groups[0][\"lr\"] # 获取当前学习率\n",
    "            elapsed = time.time() - start # 计算这40步使用的时间\n",
    "            print( # 打印信息\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time() # 重置开始实践\n",
    "            tokens = 0 # 重置token数量\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器 / Optimizer\n",
    "使用Adam优化器，Adam是一种基于一阶梯度的优化算法，结合了动量和RMSprop思想，能够自适应的调整每个参数的学习率，适用于处理大规模数据的参数优化问题。\n",
    "\n",
    "Adam更新参数的公式为：\n",
    "$$\\theta_{t} = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_{t}} + \\epsilon} \\hat{m}_{t}$$\n",
    "其中, $\\theta$是待优化参数，$\\eta$是学习率，$\\epsilon$是一个极小数，防止除以0，其余计算公式为：\n",
    "$$\\hat{m}_{t} = \\frac{m_{t}}{1 - \\beta_1^t}$$\n",
    "$$\\hat{v}_{t} = \\frac{v_{t}}{1 - \\beta_2^t}$$\n",
    "这主要是为了消除初期估计的偏差，$t$是当前迭代次数，通常，$\\beta_1=0.9$，$\\beta_2=0.999$，$\\beta_1^t,\\beta_2^t$则为相乘k次：\n",
    "$$\\beta_1^t = \\beta_1 \\times \\beta_1 \\times \\ldots \\times \\beta_1$$\n",
    "$$\\beta_2^t = \\beta_2 \\times \\beta_2 \\times \\ldots \\times \\beta_2$$\n",
    "$m_{t},v_t$则分别是Adam的一阶矩估计和二阶矩估计，$g_t$则是时间步$t$的梯度。$m_{t},v_t$计算公式为:\n",
    "$$m_{t} = \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot g$$\n",
    "$$v_{t} = \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot g^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Para\n",
    "params(iterable)：可用于迭代优化的参数或者定义参数组的dicts。\n",
    "lr (float, optional) ：学习率(默认: 1e-3)\n",
    "betas (Tuple[float, float], optional)：用于计算梯度的平均和平方的系数(默认: (0.9, 0.999))\n",
    "eps (float, optional)：为了提高数值稳定性而添加到分母的一个项(默认: 1e-8)\n",
    "weight_decay (float, optional)：权重衰减(如L2惩罚)(默认: 0)\n",
    "\"\"\"\n",
    "torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@QA \\\n",
    "@Q 优化器都存储哪些参数？以及优化器占用的显存数量如何计算？ \\\n",
    "@A 从公式中就可以看出来，在时间步$t$，计算$\\theta_t$，$m_t$，$v_t$时，都需要用到前一个时间步的数据，因此优化器中是必须存储$\\theta$，$m$，$v$这三个参数的。\n",
    "\n",
    "假设模型的参数量为1B，那么优化器就需要存储模型参数量本身$\\theta$，以及一节动量矩估计$m$和二阶动量矩估计$v$，存储的总参数量为3B，每个参数用FP32表示，一个参数占4个字节，总字节数为$3*4*10^9$，所以，大约需要11GB的内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率调整策略 / Learning rate adjustment strategy\n",
    "大语言模型在预训练阶段都通常都采用学习率调整策略，分为预热阶段和衰减阶段。预热阶段一般占整个训练步骤的 0.1% 至 0.5%，然后学习率便开始进行衰减。\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/optimizer/learning_rate_modify.png\" width=\"100%\">\n",
    "</p>\n",
    "\n",
    "本文中，学习率的调整公式为：\n",
    "$$ lrate = d_{\\text{model}}^{-0.5} \\cdot  \\min({step\\_num}^{-0.5},     {step\\_num} \\cdot {warmup\\_steps}^{-1.5}) $$\n",
    "\n",
    "这个公式可以改写成：\n",
    "$$ lrate =\\begin{cases} d_{\\text{model}}^{-0.5} \\cdot  {step\\_num} \\cdot {warmup\\_steps}^{-1.5}  (step\\_num < warmup\\_step) \\\\ d_{\\text{model}}^{-0.5} \\cdot  {step\\_num}^{-0.5} (step\\_num >= warmup\\_step) \\end{cases} $$\n",
    "\n",
    "所以， 在小于$warmup\\_steps$的step中学习率是线性增加的，然后按步数的平方根的倒数比例降低学习率。本文中，$warmup\\_steps=4000$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Caution\n",
    "还需要注意的是，当$step$为0时，学习率不能为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@QA \\\n",
    "@Q 为什么要采用学习率调整策略？ \\\n",
    "@A 在模型训练的初始阶段，由于参数是随机初始化的，梯度通常也比较大，因此需要使用较小的学习率使得训练较为稳定。训练中通常采用线性预热策略来逐步调整学习率。具体来说，学习率将从一个非常小的数值（例如 0 或者 $1×10^{−8}$）线性平稳增加，直到达到预设的最大阈值。模型在学习率较大时可以加快收敛速度，这个最大阈值通常设定在 $5×10^{−5}$ 到 $1×10^{−4}$ 之间。达到最大阈值之后学习率会开始逐渐衰减，以避免在较优点附近来回震荡。最后，学习率一般会衰减到其最大阈值的 10%。常见的衰减策略有线性衰减，余弦衰减，平方根倒数衰减。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样例测试\n",
    "下面，我们通过三个样例来看学习率的调整过程，具体过程看代码和注释。这里只对LambdaLR进行解读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"学习率调整器\n",
    "LambdaLR的作用就是自定义一个函数，然后根据这个函数来调整优化器的学习率，从下面代码可以看出，Adam优化器的学习率初始值为1，lr_lambda函数输出是一个学习率的倍数，这个倍数会乘以当前的学习率，然后更新优化器的学习率。\n",
    "\n",
    "参数：\n",
    "optimizer：被调整学习率的优化器\n",
    "lr_lambda：用户自定义的学习率调整规则。可以是lambda表达式，也可以是函数\n",
    "last_epoch：当前优化器的已迭代次数，后文我们将其称为epoch计数器。默认是-1，字面意思是第-1个epoch已完成，也就是当前epoch从0算起，从头开始训练。如果是加载checkpoint继续训练，那么这里要传入对应的已迭代次数\n",
    "verbose：是否在更新学习率时在控制台输出提醒\n",
    "\"\"\"\n",
    "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch\n",
    "import pandas as pd\n",
    "def example_learning_schedule():\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # model_size=512, factor=1, warmup_steps=4000\n",
    "        [512, 1, 8000],  # model_size=512, factor=1, warmup_steps=8000\n",
    "        [256, 1, 4000],  # model_size=256, factor=1, warmup_steps=4000\n",
    "    ]\n",
    "\n",
    "    dummy_model = torch.nn.Linear(1, 1) # 定义一个简单的线性模型\n",
    "    learning_rates = []\n",
    "\n",
    "    # we have 3 examples in opts list.\n",
    "    for idx, example in enumerate(opts):\n",
    "        # run 20000 epoch for each example\n",
    "        optimizer = torch.optim.Adam( # 定义优化器\n",
    "            dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n",
    "        )\n",
    "        lr_scheduler = LambdaLR( # 定义学习率调整器\n",
    "            optimizer=optimizer, lr_lambda=lambda step: rate(step, *example)\n",
    "        )\n",
    "        tmp = []\n",
    "        for step in range(20000): # 进行20000步训练, 记录每一步的学习率\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"]) # 记录当前学习率\n",
    "            optimizer.step() # 更新参数\n",
    "            lr_scheduler.step() # 更新学习率\n",
    "        learning_rates.append(tmp) # 记录当前example的学习率\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates) # 转换为tensor\n",
    "\n",
    "    # Enable altair to handle more than 5000 rows\n",
    "    alt.data_transformers.disable_max_rows() # 禁用最大行数限制，使altair可以处理超过5000行的数据\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "        .mark_line()\n",
    "        .properties(width=600)\n",
    "        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "example_learning_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行之后，可以看到学习率的调整曲线。\n",
    "<p align=\"center\">\n",
    "    <img src=\"imgs/optimizer/lrate_alter_visualization.png\" width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化 / Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@QA\n",
    "@Q 什么是正则化？\n",
    "@A 训练模型的本质是可以看成寻找一个函数$H_0(x)$来你和数据集，而正则化的目的就是防止模型过拟合，增强模型的泛化能力。简单来说，正则化就是给**损失函数**增加一个正则项来限制损失函数的增加。\n",
    "\n",
    "正则化相关问题后续单独开一篇文章来讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label smoothing 标签平滑\n",
    "标签平滑（Label Smoothing）是一种在机器学习中常用的正则化技术，特别是在分类任务中。它的核心思想是将硬标签（hard labels）转换为软标签（soft labels），以此来防止模型在训练过程中对某一类别的预测过于自信，从而提高模型的泛化能力。\n",
    "\n",
    "在传统的分类任务中，我们通常使用one-hot编码来表示标签，即目标类别的概率为1，其他类别的概率为0。这种表示方法称为硬标签。然而，硬标签可能导致模型在训练数据上过拟合，特别是当训练数据无法覆盖所有情况时。为了解决这个问题，标签平滑通过在真实标签的概率上引入一个小的噪声$\\varepsilon$，将其从1降低到$1-\\varepsilon$，同时将其他标签的概率从0提高到$\\varepsilon/K$，其中K是类别总数。这样，每个标签的概率分布变得更加平滑，模型对于每个类别的预测不再那么绝对。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本文中，则是使用了KL散度损失来实现标签平滑。\n",
    "\n",
    "## KL散度损失\n",
    "KL散度可以用来衡量两个概率分布之间的相似度，KL散度越小，说明两个概率分布的距离越近，越相似。\n",
    "\n",
    "计算公式为：\n",
    "$$L(y_{pred}, y_{true}) = y_{true} \\cdot log{\\frac{y_{true}}{y_{pred}}} = y_{true} \\cdot (\\log y_{true} - \\log y_{pred}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Para\n",
    "size_average与reduce 已被弃用，具体功能由参数reduction代替\n",
    "reduction：指定损失输出的形式，有四种选择：none|mean|batchmean|sum。none：损失不做任何处理，直接输出一个数组；mean：将得到的损失求平均值再输出，会输出一个数；batchmean：将输出的总和除以batchsize；sum：将得到的损失求和再输出，会输出一个数\n",
    "log_target：指定是否对输入的y使用log操作\n",
    "\"\"\"\n",
    "torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0): # size是词表大小，padding_idx是padding的索引，smoothing是平滑参数\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone() # kldivloss的输入是log_softmax\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2)) # 除了padding_idx和正确的标签，其他的概率都是smoothing/size-2\n",
    "        \"\"\"scatter_函数是一个用于在特定索引处更新张量的原地操作。这个函数接受三个参数：dim、index和src。dim参数指定了要沿着哪个维度进行索引，index是一个包含索引的张量，而src是包含要散布的值的张量。\n",
    "        \"\"\"\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        print(f\"true_dist:{true_dist}\")\n",
    "        # 将正确的标签的概率设置为confidence\n",
    "        true_dist[:, self.padding_idx] = 0 # 将padding_idx的概率设置为0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx) # 获取padding的位置\n",
    "        print(f\"mask:{mask}\")\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0) # 将padding的概率设置为0\n",
    "        self.true_dist = true_dist\n",
    "        print(f\"true_dist:{true_dist}\")\n",
    "        return self.criterion(x, true_dist.clone().detach()) # 计算损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Caution \n",
    "一定要理解这里padding的含义，词表大小为$vocab_{size}$，那么padding_idx就是对第padding_idx个token进行mask操作，也就是one-hot编码中的下标。对padding_idx位置的token不作预测，所以需要把每一个真实值$y_{true}$的one-hot向量中的padding_idx位置的概率设为0，同时，真实值为padding_idx的样例也要mask掉。\n",
    "\n",
    "具体过程可以通过下面的测试样例来debug一遍，看true_dist的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样例测试\n",
    "下面使用一个样例来测试Label smoothing，下面这个代码主要是展示了label 平滑之后的结果，label就是真实值。每个$y_{true}$用one-hot表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_label_smoothing():\n",
    "    crit = LabelSmoothing(5, 0, 0.4) # 定义一个LabelSmoothing对象\n",
    "    predict = torch.FloatTensor( # 定义一个预测值\n",
    "        [\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "            [0, 0.2, 0.7, 0.1, 0],\n",
    "        ]\n",
    "    )\n",
    "    print(predict.log())\n",
    "    crit(x=predict.log(), target=torch.LongTensor([2, 1, 0, 3, 3]))\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"target distribution\": crit.true_dist[x, y].flatten(),\n",
    "                    \"columns\": y,\n",
    "                    \"rows\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(5)\n",
    "            for x in range(5)\n",
    "        ]\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect(color=\"Blue\", opacity=1)\n",
    "        .properties(height=200, width=200)\n",
    "        .encode(\n",
    "            alt.X(\"columns:O\", title=None),\n",
    "            alt.Y(\"rows:O\", title=None),\n",
    "            alt.Color(\n",
    "                \"target distribution:Q\", scale=alt.Scale(scheme=\"viridis\")\n",
    "            ),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "example_label_smoothing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出结果为：\n",
    "<p align=\"center\">\n",
    "    <svg src=\"imgs/optimizer/label_smoothing_true_dist.svg\">\n",
    "</p>\n",
    "图中，横轴表示one-hot向量的下标，纵轴表示第i个token的one-hot向量，黄色位置就是真实值$y_{true}$，蓝色位置是smooth之后的值，为0.1333。每个one-hot向量的0位置，以及$y_{true}=0$的向量都被mask了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面再通过一个示例来看Label smoothing对loss的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, crit):\n",
    "    d = x + 3 * 1 # x/d = x/(x+3) =1-3/(x+3)\n",
    "    # 所以，当x越大，x/d越接近1，那么predict就越接近[0, 1, 0, 0, 0]，损失越小\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Loss\": [loss(x, crit) for x in range(1, 100)], # 计算每一步的损失\n",
    "            \"Steps\": list(range(99)),\n",
    "        }\n",
    "    ).astype(\"float\")\n",
    "\n",
    "    return (\n",
    "        alt.Chart(loss_data)\n",
    "        .mark_line()\n",
    "        .properties(width=350)\n",
    "        .encode(\n",
    "            x=\"Steps\",\n",
    "            y=\"Loss\",\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "penalization_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果为：\n",
    "<p align=\"center\">\n",
    "    <svg src=\"imgs/optimizer/label_smoothing_loss_res.svg\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
