> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.nowcoder.com](https://www.nowcoder.com/discuss/470889397320257536)

学习资料（讲的很细很清楚）：
==============

[一个框架看懂优化算法之异同 SGD/AdaGrad/Adam](https://gw-c.nowcoder.com/api/sparta/jump/link?link=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F32230623)[Adam那么棒，为什么还对SGD念念不忘 (3)—— 优化算法的选择与使用策略](https://gw-c.nowcoder.com/api/sparta/jump/link?link=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F32338983)

[(1条消息) 机器学习11种优化器推导过程详解(SGD,BGD,MBGD,Momentum,NAG,Adagrad,Adadelta,RMSprop,Adam,Nadma,Adamx)_日拱一两卒的博客-CSDN博客_机器学习优化器](https://gw-c.nowcoder.com/api/sparta/jump/link?link=https%3A%2F%2Fblog.csdn.net%2Fyangwohenmai1%2Farticle%2Fdetails%2F124882119)

总结
==

优化器的作用：更新和计算影响模型训练和模型输出的网络参数，使其逼近或达到最优值，从而最小化（或最大化）损失函数。

发展路径：SGD -> SGDM -> NAG -> AdaGrad -> AdaDelta / RMSProp -> Adam -> Nadam -> AdamW

发展阶段：

*   没有动量：梯度下降<计算所有样本的梯度平均> / SGD<随机选择一个样本计算梯度> / 小批量随机梯度下降<每次处理一批样本>
*   一阶动量（惯性）：SGDM<梯度更新方向由历史梯度更新方向和当前梯度更新方向共同决定> / NAG<假设参数先按上一轮梯度更新方向变化，再计算当前梯度更新方向>
*   二阶动量（自适应学习率）：AdaGrad<迄今为止所有梯度的平方和> / AdaDelta / RMSProp<只关注过去一段时间内的梯度更新频率>
*   一阶动量+二阶动量：Adam<SGDM+AdaDelta> / Nadam<NAG+AdaDelta> / AdamW<Adam+L2正则化>

![](https://uploadfiles.nowcoder.com/images/20230330/485173046_1680140690929/D2B5CA33BD970F64A6301FA75AE2EB22)

![](https://uploadfiles.nowcoder.com/images/20230330/485173046_1680140726796/D2B5CA33BD970F64A6301FA75AE2EB22)

各种优化器如何选择
=========

1.  刚入门选NAG或Adam
2.  如果**模型非常稀疏**，优先考虑**自适应学习率**的算法
3.  随机梯度下降算法的收敛速度和数据集大小的关系不大。因此，可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，然后通过参数搜索的方式寻找最优的训练参数。

等等。

BERT使用的优化器是什么？Adam相较于传统的SGD的优点是什么？（超参数一面）
=========================================

BERT使用的是Adam优化器。Adam结合SGDM和AdaDelta两种优化算法的优点。对梯度的一阶动量（惯性）和二阶动量（更新频率）进行综合考虑，计算出更新步长。**一阶动量**的优势在于他能够学习到历史梯度下降的惯性，避免受到单个样本分布的干扰，**减少震荡**，加快收敛；**二阶动量**的优势在于是**自适应学习率**，为参数的不同维分配不同的学习率，在**模型稀疏的情况下效果很好**。

PS：Notion的笔记复制过来格式会变，这次就用截图吧，希望能看清楚。。。

[#算法面经#](/creation/subject/30139c431b104498941e79180f7a9037)