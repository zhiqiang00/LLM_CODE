åœ¨è¿‡å»å‡ å¹´ä¸­ï¼Œæ·±åº¦å­¦ä¹ çš„è¿›æ­¥è¡¨æ˜ï¼Œéšç€è®­ç»ƒæ—¶é—´ã€æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å¤§å°çš„å¢åŠ ï¼Œå¤§å‹æ¨¡å‹çš„æ€§èƒ½æŒç»­æ”¹å–„ç†è§£è¿™äº›ç³»ç»Ÿåœ¨èµ„æºå¢åŠ æ—¶çš„æ”¹è¿›é€Ÿç‡å¯¹äºå¸Œæœ›ä»¥è®¡ç®—æœ€ä¼˜çš„æ–¹å¼æ‰©å±•æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´çš„äººæ¥è¯´è‡³å…³é‡è¦ã€‚ç»éªŒä¸Šï¼Œå‡ é¡¹ç ”ç©¶å‘ç°ï¼Œå¯¹äºå…·æœ‰ğ‘¡æ¬¡æ›´æ–°æ­¥éª¤å’Œğ‘ä¸ªå‚æ•°çš„ç½‘ç»œï¼Œå…¶æŸå¤±å¯ä»¥ç›¸å¯¹å‡†ç¡®åœ°è¿‘ä¼¼ä¸ºè¿™äº›é‡ä¸­çš„å¹‚å¾‹ã€‚

$$\mathcal L = C_t \ t^{-\alpha_t} + C_N N^{-\alpha_N} + \mathcal L_{\infty}.$$

å¸¸æ•° $C_t, C_N$ å’ŒæŒ‡æ•° $\alpha_t$ å’Œ $\alpha_N$ ä»¥åŠæ¸è¿‘çº¿ $\mathcal L_{\infty}$ å–å†³äºå­¦ä¹ ä»»åŠ¡å’Œç¥ç»ç½‘ç»œæ¶æ„ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸¤ä¸ªç®€å•çš„ä¾‹å­ï¼Œåˆ†åˆ«æ˜¯åœ¨è§†è§‰ä»»åŠ¡ï¼ˆä½¿ç”¨å·ç§¯ç½‘ç»œçš„ CIFAR-5Mï¼‰å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼ˆä½¿ç”¨Transformerçš„ Wikitext-103 è¯­è¨€å»ºæ¨¡ï¼‰ä¸­ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡å¢åŠ å®½åº¦ $N$ æ¥æ”¹å˜æ¨¡å‹çš„å‚æ•°æ•°é‡ã€‚


![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-7.49.23-AM-1024x480.png) ![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-7.49.33-AM-1024x578.png)

æˆ‘ä»¬çœ‹åˆ°ï¼Œéšç€æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒæ­¥æ•°çš„å¢åŠ ï¼Œæ€§èƒ½å®šæœŸæé«˜ã€‚å°½ç®¡æˆ‘ä»¬çš„ [ä¸Šä¸€ç¯‡åšå®¢æ–‡ç« ](https://kempnerinstitute.harvard.edu/research/deeper-learning/infinite-limits-of-neural-networks/) è®¨è®ºäº†ç¥ç»ç½‘ç»œçš„æ— é™å‚æ•°æé™ ğ‘â†’âˆï¼Œä½†ç†è§£æ‰©å±•æ³•åˆ™éœ€è¦è¡¨å¾æœ‰é™æ¨¡å‹è§„æ¨¡çš„å½±å“ï¼Œè¿™äº›å½±å“åœ¨è¶³å¤Ÿçš„è®­ç»ƒåé™åˆ¶äº†æ€§èƒ½ã€‚

Additional Observed Phenomena Related to Scaling Laws  
ä¸ç¼©æ”¾æ³•åˆ™ç›¸å…³çš„é¢å¤–è§‚å¯Ÿç°è±¡
----------------------------------------------------------------------

In addition to the simple power-law neural scaling laws in training time and model size that are observed in many scaling law studies, we would also like to explain some other observed effects in network training. Some key empirical observations include  
é™¤äº†è®¸å¤šç¼©æ”¾æ³•ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°çš„è®­ç»ƒæ—¶é—´å’Œæ¨¡å‹è§„æ¨¡çš„ç®€å•å¹‚å¾‹ç¥ç»ç¼©æ”¾æ³•åˆ™å¤–ï¼Œæˆ‘ä»¬è¿˜æƒ³è§£é‡Šä¸€äº›åœ¨ç½‘ç»œè®­ç»ƒä¸­è§‚å¯Ÿåˆ°çš„å…¶ä»–æ•ˆåº”ã€‚ä¸€äº›å…³é”®çš„ç»éªŒè§‚å¯ŸåŒ…æ‹¬

1.  **Asymmetric exponents**: the model size and training time exponents $\alpha_N , \alpha_t$ are often different[1-6](#references).  
    **éå¯¹ç§°æŒ‡æ•°**ï¼šæ¨¡å‹å¤§å°å’Œè®­ç»ƒæ—¶é—´çš„æŒ‡æ•° $\alpha_N , \alpha_t$ é€šå¸¸æ˜¯ä¸åŒçš„[1-6](#references)ã€‚
2.  **Data reuse effects**: Reusing data leads to a gradual buildup of the gap between train loss and test loss compared to the online training regime where data is never repeated and train and test losses are identical[[10](#footnote_9_8573 "Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good online learners are good offline generalizers. International Conference on Learning Representations, 2021")][[11](#footnote_10_8573 "Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing systems, 2023")].  
    **æ•°æ®é‡ç”¨çš„å½±å“**ï¼šä¸ä»ä¸é‡å¤æ•°æ®çš„åœ¨çº¿è®­ç»ƒæ–¹å¼ç›¸æ¯”ï¼Œé‡ç”¨æ•°æ®ä¼šå¯¼è‡´è®­ç»ƒæŸå¤±å’Œæµ‹è¯•æŸå¤±ä¹‹é—´çš„å·®è·é€æ¸åŠ å¤§ï¼Œè®­ç»ƒå’Œæµ‹è¯•æŸå¤±ç›¸åŒ[[10](#footnote_9_8573 "Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good online learners are good offline generalizers. International Conference on Learning Representations, 2021")][[11](#footnote_10_8573 "Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing systems, 2023")]ã€‚
3.  **Change in rate of convergence to large width limit**: Very early in training networks converge at a rate $1/\text{width}$ to their infinite width limit [[12](#footnote_11_8573 "Blake Bordelon and Cengiz Pehlevan. Dynamics of finite width kernel and prediction fluctuations in mean field neural networks. Advances in Neural Information Processing Systems, 2023.")][[13](#footnote_12_8573 "Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan. Feature-learning networks are consistent across widths at realistic scales, Advances in Neural Information Processing Systems 2023.")]Late in training, the network has a loss that scales as $\text{width}^{-c}$ where $c$ is task and architecture dependnent[12](#references), 13[[14](#footnote_13_8573 "Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. arXiv preprint arXiv:2102.06701, 2021.")].  
    **æ”¶æ•›é€Ÿç‡å˜åŒ–åˆ°å¤§å®½åº¦æé™**: åœ¨è®­ç»ƒçš„æ—©æœŸï¼Œç½‘ç»œä»¥ $1/\text{width}$ çš„é€Ÿç‡æ”¶æ•›åˆ°å®ƒä»¬çš„æ— é™å®½åº¦æé™ï¼Œåœ¨è®­ç»ƒçš„åæœŸï¼Œç½‘ç»œçš„æŸå¤±å‘ˆç°ä¸º $\text{width}^{-c}$ çš„æ¯”ä¾‹ï¼Œå…¶ä¸­ $c$ ä¾èµ–äºä»»åŠ¡å’Œæ¶æ„.
4.  **Compute suboptimality of ensembling**: On large datasets or in online training, ensembling over many randomly initialized models often fails to match the performance of training a single larger model.[13](#references)  
    **è®¡ç®—é›†åˆçš„æ¬¡ä¼˜æ€§**ï¼šåœ¨å¤§æ•°æ®é›†æˆ–åœ¨çº¿è®­ç»ƒä¸­ï¼Œè®¸å¤šéšæœºåˆå§‹åŒ–æ¨¡å‹çš„é›†åˆé€šå¸¸æ— æ³•è¾¾åˆ°è®­ç»ƒå•ä¸ªæ›´å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚[13](#references)

Below we describe a very simple model of network training dynamics that can reproduce these effects.  
ä¸‹é¢æˆ‘ä»¬æè¿°ä¸€ä¸ªéå¸¸ç®€å•çš„ç½‘ç»œè®­ç»ƒåŠ¨æ€æ¨¡å‹ï¼Œå¯ä»¥é‡ç°è¿™äº›æ•ˆæœã€‚

A Model of Compute Optimal Scaling Laws  
è®¡ç®—æœ€ä¼˜ç¼©æ”¾æ³•åˆ™çš„æ¨¡å‹
-----------------------------------------------------

We seek the simplest possible model that captures all of these observed phenomena. In the [previous blog post](https://kempnerinstitute.harvard.edu/research/deeper-learning/infinite-limits-of-neural-networks/), we introduced the kernel limit of neural networks which arises from randomly initializing large width networks in a certain parameterization. This model has serious deficiencies as a model of neural network dynamics since the internal representations in the network are static throughout learning, however it is much more analytically tractable since it is essentially a linear model. Despite the deficiencies of this kernel (linear model) regime of neural network training, we will show that all of the above neural scaling law effects are already observable in the learning dynamics of a linear model. We therefore aim to characterize the test and train loss dynamics of this kind of model.  
æˆ‘ä»¬å¯»æ±‚å°½å¯èƒ½ç®€å•çš„æ¨¡å‹ï¼Œä»¥æ•æ‰æ‰€æœ‰è¿™äº›è§‚å¯Ÿåˆ°çš„ç°è±¡ã€‚åœ¨[ä¹‹å‰çš„åšå®¢æ–‡ç« ](https://kempnerinstitute.harvard.edu/research/deeper-learning/infinite-limits-of-neural-networks/)ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç¥ç»ç½‘ç»œçš„æ ¸æé™ï¼Œè¿™ä¸€ç°è±¡æºäºåœ¨æŸç§å‚æ•°åŒ–ä¸‹éšæœºåˆå§‹åŒ–å¤§å®½åº¦ç½‘ç»œã€‚ä½œä¸ºç¥ç»ç½‘ç»œåŠ¨æ€çš„æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹å­˜åœ¨ä¸¥é‡çš„ç¼ºé™·ï¼Œå› ä¸ºç½‘ç»œä¸­çš„å†…éƒ¨è¡¨å¾åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ˜¯é™æ€çš„ï¼Œç„¶è€Œï¼Œç”±äºå®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå› æ­¤è§£ææ€§æ›´å¼ºã€‚å°½ç®¡è¿™ä¸ªæ ¸ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰æ¨¡å¼çš„ç¥ç»ç½‘ç»œè®­ç»ƒå­˜åœ¨ç¼ºé™·ï¼Œæˆ‘ä»¬å°†è¯æ˜ä¸Šè¿°æ‰€æœ‰ç¥ç»æ‰©å±•å®šå¾‹æ•ˆåº”å·²ç»åœ¨çº¿æ€§æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ä¸­å¯è§‚å¯Ÿåˆ°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯æè¿°è¿™ç§æ¨¡å‹çš„æµ‹è¯•å’Œè®­ç»ƒæŸå¤±åŠ¨æ€ã€‚

We consider a network with ğ‘ trainable parameters, ğ‘ƒ data points, and ğ‘¡ timesteps of training. Our goal is to characterize the expected or typical test error as a function of these quantities over random draws of datasets and initial network features.  
æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå…·æœ‰ ğ‘ ä¸ªå¯è®­ç»ƒå‚æ•°ã€ğ‘ƒ ä¸ªæ•°æ®ç‚¹å’Œ ğ‘¡ ä¸ªè®­ç»ƒæ—¶é—´æ­¥çš„ç½‘ç»œã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†é¢„æœŸæˆ–å…¸å‹æµ‹è¯•è¯¯å·®ä½œä¸ºè¿™äº›æ•°é‡çš„å‡½æ•°ï¼Œå¯¹æ•°æ®é›†å’Œåˆå§‹ç½‘ç»œç‰¹å¾çš„éšæœºæŠ½æ ·è¿›è¡Œç‰¹å¾åŒ–ã€‚

A Linear Model for Compute-Optimal Scaling  
è®¡ç®—æœ€ä¼˜æ‰©å±•çš„çº¿æ€§æ¨¡å‹
--------------------------------------------------------

Neural networks in certain limits can operate as linear models. In this regime, the output prediction $f$ of the neural network is a linear combination of its $N$ features $\left\{\tilde{\psi}_k(x) \right\}_{k=1}^N$, which arise from a rank-$N$ kernel associated with an $N$ parameter model. The target function, $y$, on the other hand, is a linear combination of a complete set of features $\left\{ \psi_k(x) \right\}_{k=1}^\infty$, corresponding to a complete set of square integrable functions. These expansions take the form $$f(x) = \sum_{k} \tilde{\psi}_k(x) w_k \ , \ y(x) = \sum_k \psi_k(x) w^\star_k.$$ We will use the basis of features $\psi_k(x)$ as the infinite width kernel eigenfunctions \.[8,10](#references) The finite modelâ€™s $N$ features $\{ \tilde{\psi}_k \}_{k=1}^N$ can be expanded in the basis of the original features with coefficients $A_{k\ell}$, $$\tilde{\psi}_k(x) = \sum_{\ell = 1}^\infty A_{k\ell} \ \psi_\ell(x) .$$  
åœ¨æŸäº›é™åˆ¶æ¡ä»¶ä¸‹ï¼Œç¥ç»ç½‘ç»œå¯ä»¥ä½œä¸ºçº¿æ€§æ¨¡å‹è¿ä½œã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¥ç»ç½‘ç»œçš„è¾“å‡ºé¢„æµ‹ $f$ æ˜¯å…¶ $N$ ä¸ªç‰¹å¾ $\left\{\tilde{\psi}_k(x) \right\}_{k=1}^N$ çš„çº¿æ€§ç»„åˆï¼Œè¿™äº›ç‰¹å¾æºè‡ªä¸ $N$ å‚æ•°æ¨¡å‹ç›¸å…³çš„ç§©ä¸º $N$ çš„æ ¸ã€‚å¦ä¸€æ–¹é¢ï¼Œç›®æ ‡å‡½æ•° $y$ æ˜¯ä¸ä¸€ç»„å®Œæ•´çš„å¹³æ–¹å¯ç§¯å‡½æ•°å¯¹åº”çš„å®Œæ•´ç‰¹å¾é›† $\left\{ \psi_k(x) \right\}_{k=1}^\infty$ çš„çº¿æ€§ç»„åˆã€‚è¿™äº›å±•å¼€çš„å½¢å¼ä¸º $$f(x) = \sum_{k} \tilde{\psi}_k(x) w_k \ , \ y(x) = \sum_k \psi_k(x) w^\star_k.$$ æˆ‘ä»¬å°†ä½¿ç”¨ç‰¹å¾åŸº $\psi_k(x)$ ä½œä¸ºæ— é™å®½åº¦æ ¸çš„ç‰¹å¾å‡½æ•° \.[8,10](#references) æœ‰é™æ¨¡å‹çš„ $N$ ä¸ªç‰¹å¾ $\{ \tilde{\psi}_k \}_{k=1}^N$ å¯ä»¥ç”¨åŸå§‹ç‰¹å¾çš„åŸºå±•å¼€ï¼Œç³»æ•°ä¸º $A_{k\ell}$ï¼Œå³ $$\tilde{\psi}_k(x) = \sum_{\ell = 1}^\infty A_{k\ell} \ \psi_\ell(x) .$$

We will model the matrix $A_{k\ell}$ as random, which reflects the fact that the empirical kernel in a finite parameter model depends on the random initialization of the network weights. The statics of this model were analyzed in prior works [[15](#footnote_14_8573 "Alexander Maloney, Daniel Roberts, James Sully. A solvable model of neural scaling laws. arXiv preprint arXiv:2210.16859. 2022.")][[16](#footnote_15_8573 "Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, Cengiz Pehlevan. Onset of Variance-limited Behavior for networks in the lazy and rich regimes. arXiv preprint arXiv:2212.12147. 2022.")], but in this work we focus on the dynamics of training.  
æˆ‘ä»¬å°†æŠŠçŸ©é˜µ $A_{k\ell}$ å»ºæ¨¡ä¸ºéšæœºçŸ©é˜µï¼Œè¿™åæ˜ äº†æœ‰é™å‚æ•°æ¨¡å‹ä¸­çš„ç»éªŒæ ¸ä¾èµ–äºç½‘ç»œæƒé‡çš„éšæœºåˆå§‹åŒ–ã€‚è¯¥æ¨¡å‹çš„é™æ€åœ¨ä¹‹å‰çš„å·¥ä½œä¸­è¿›è¡Œäº†åˆ†æ [[15](#footnote_14_8573 "Alexander Maloney, Daniel Roberts, James Sully. A solvable model of neural scaling laws. arXiv preprint arXiv:2210.16859. 2022.")][[16](#footnote_15_8573 "Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, Cengiz Pehlevan. Onset of Variance-limited Behavior for networks in the lazy and rich regimes. arXiv preprint arXiv:2212.12147. 2022.")]ï¼Œä½†åœ¨æœ¬å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºè®­ç»ƒçš„åŠ¨æ€ã€‚

To train the model parameters $w_k$ with gradient based training, we randomly sample a training set with $P$ data points ${ x_\mu }_{\mu=1}^P$ drawn from the population distribution and train the model with gradient descent/gradient flow on the training loss $\hat{\mathcal{L}} = \frac{1}{P} \sum_{\mu=1}^P [f(x_\mu) â€“ y(x_\mu)]^2$. For gradient flow, we have  
ä¸ºäº†ä½¿ç”¨åŸºäºæ¢¯åº¦çš„è®­ç»ƒæ¥è®­ç»ƒæ¨¡å‹å‚æ•° $w_k$ï¼Œæˆ‘ä»¬éšæœºæŠ½å–ä¸€ä¸ªåŒ…å« $P$ ä¸ªæ•°æ®ç‚¹çš„è®­ç»ƒé›† ${ x_\mu }_{\mu=1}^P$ï¼Œè¯¥æ•°æ®ç‚¹æ¥è‡ªäºæ€»ä½“åˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨è®­ç»ƒæŸå¤± $\hat{\mathcal{L}} = \frac{1}{P} \sum_{\mu=1}^P [f(x_\mu) â€“ y(x_\mu)]^2$ è¿›è¡Œæ¢¯åº¦ä¸‹é™/æ¢¯åº¦æµè®­ç»ƒæ¨¡å‹ã€‚å¯¹äºæ¢¯åº¦æµï¼Œæˆ‘ä»¬æœ‰

$$\frac{d}{dt} \mathbf w(t) = â€“ \eta \nabla \hat{\mathcal L}(\mathbf w(t)) . $$

For simplicity in this post we focus on gradient flow, but discrete time algorithms such as gradient descent or momentum and one pass SGD can also be handled in our framework, see our paper.[7](#references)  
ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åœ¨è¿™ç¯‡æ–‡ç« ä¸­ä¸“æ³¨äºæ¢¯åº¦æµï¼Œä½†ç¦»æ•£æ—¶é—´ç®—æ³•ï¼Œå¦‚æ¢¯åº¦ä¸‹é™ã€åŠ¨é‡å’Œä¸€æ¬¡æ€§éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ä¹Ÿå¯ä»¥åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­å¤„ç†ï¼Œè¯¦è§æˆ‘ä»¬çš„è®ºæ–‡ã€‚[7](#references)

Our goal is to track the test error $\mathcal L =\mathbb{E}_{x} [f(x) â€“ y(x)]^2$ over training time. Since $f(x,t)$ depends on the random dataset and random projection, we have to develop a method to average over these sources of disorder.  
æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è·Ÿè¸ªæµ‹è¯•è¯¯å·® $\mathcal L =\mathbb{E}_{x} [f(x) â€“ y(x)]^2$ éšè®­ç»ƒæ—¶é—´çš„å˜åŒ–ã€‚ç”±äº $f(x,t)$ ä¾èµ–äºéšæœºæ•°æ®é›†å’ŒéšæœºæŠ•å½±ï¼Œæˆ‘ä»¬å¿…é¡»å¼€å‘ä¸€ç§æ–¹æ³•æ¥å¯¹è¿™äº›æ— åºæºè¿›è¡Œå¹³å‡ã€‚

Dynamical Mean Field Theory for Learning Curves  
å­¦ä¹ æ›²çº¿çš„åŠ¨æ€å‡åœºç†è®º
-------------------------------------------------------------

We develop a theory to track the test and train loss dynamics in this random feature model for ğ‘, ğ‘ƒ large. To analytically calculate these losses, we utilize ideas from statistical physics, specifically dynamical mean field theory (DMFT). This method summarizes all relevant summary statistics of the network in terms of correlation and response functions.  
æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç†è®ºï¼Œä»¥è·Ÿè¸ªåœ¨è¿™ä¸ªéšæœºç‰¹å¾æ¨¡å‹ä¸­ï¼Œğ‘å’Œğ‘ƒå¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œæµ‹è¯•å’Œè®­ç»ƒæŸå¤±çš„åŠ¨æ€å˜åŒ–ã€‚ä¸ºäº†åˆ†ææ€§åœ°è®¡ç®—è¿™äº›æŸå¤±ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ç»Ÿè®¡ç‰©ç†çš„æ€æƒ³ï¼Œç‰¹åˆ«æ˜¯åŠ¨æ€å‡åœºç†è®ºï¼ˆDMFTï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç›¸å…³æ€§å’Œå“åº”å‡½æ•°æ¥æ€»ç»“ç½‘ç»œçš„æ‰€æœ‰ç›¸å…³ç»Ÿè®¡é‡ã€‚

Below, we plot an example of our theoretical predictions of test loss (dashed black lines) against experimental training (solid) for feature maps of varying dimension $N$ with large dataset size $P=1000$. Standard deviations over random realizations of the dataset and projection matrix $A$ are plotted as bands of shaded color. We see that the theory (dashed black lines) accurately captures the deviation of finite models from the $N,P \to \infty$ limiting dynamics (blue). Further, increasing training time $t$ and increasing model size $N$ leads to consistent reductions in test loss.  
ä¸‹é¢ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†æˆ‘ä»¬çš„ç†è®ºé¢„æµ‹çš„æµ‹è¯•æŸå¤±ï¼ˆè™šçº¿é»‘è‰²çº¿æ¡ï¼‰ä¸å®éªŒè®­ç»ƒï¼ˆå®çº¿ï¼‰ä¹‹é—´çš„ç¤ºä¾‹ï¼Œç‰¹å¾å›¾çš„ç»´åº¦ $N$ å˜åŒ–ï¼Œæ•°æ®é›†å¤§å°ä¸º $P=1000$ã€‚æ•°æ®é›†å’ŒæŠ•å½±çŸ©é˜µ $A$ éšæœºå®ç°çš„æ ‡å‡†å·®è¢«ç»˜åˆ¶ä¸ºé˜´å½±é¢œè‰²çš„å¸¦çŠ¶ã€‚æˆ‘ä»¬çœ‹åˆ°ç†è®ºï¼ˆè™šçº¿é»‘è‰²çº¿æ¡ï¼‰å‡†ç¡®åœ°æ•æ‰åˆ°äº†æœ‰é™æ¨¡å‹ä¸ $N,P \to \infty$ é™åˆ¶åŠ¨æ€ï¼ˆè“è‰²ï¼‰ä¹‹é—´çš„åå·®ã€‚æ­¤å¤–ï¼Œå¢åŠ è®­ç»ƒæ—¶é—´ $t$ å’Œå¢åŠ æ¨¡å‹å¤§å° $N$ ä¼šå¯¼è‡´æµ‹è¯•æŸå¤±çš„ä¸€è‡´å‡å°‘ã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.26.50-AM-1024x735.png)

However, if the dataset size is small, the returns to increasing model size eventually diminish as the test loss is bottlenecked by the amount of available data. Below we plot varying model sizes $N$ as we train on a dataset of size $P=128$.  
ç„¶è€Œï¼Œå¦‚æœæ•°æ®é›†çš„è§„æ¨¡å¾ˆå°ï¼Œå¢åŠ æ¨¡å‹å¤§å°çš„æ”¶ç›Šæœ€ç»ˆä¼šå‡å°ï¼Œå› ä¸ºæµ‹è¯•æŸå¤±å—åˆ°å¯ç”¨æ•°æ®é‡çš„ç“¶é¢ˆã€‚ä¸‹é¢æˆ‘ä»¬ç»˜åˆ¶äº†åœ¨æ•°æ®é›†å¤§å°ä¸º $P=128$ æ—¶ï¼Œä¸åŒæ¨¡å‹å¤§å° $N$ çš„è®­ç»ƒæƒ…å†µã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.26.59-AM-1024x712.png)

Power Law Bottleneck Scalings  
å¹‚å¾‹ç“¶é¢ˆç¼©æ”¾
--------------------------------------

From the last section, we saw that the performance of the model can be bottlenecked by one of the three computational/statistical resources: training time $t$, model size $N$, and total available data $P$. By this we mean that even if the other two resources were effectively infinite, the loss can still be nonzero because of the finite value of the third quantity. In this section, we show that the dependence of the loss on these resources can obey power laws when the features themselves have power-law structure. It has been observed that the spectra of neural network kernels on real datasets often follow power-laws[14](#references) [[17](#footnote_16_8573 "Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In International Conference on Machine Learning, pp. 1024â€“1034. PMLR, 2020.")]  
ä»ä¸Šä¸€èŠ‚ï¼Œæˆ‘ä»¬çœ‹åˆ°æ¨¡å‹çš„æ€§èƒ½å¯èƒ½ä¼šå—åˆ°ä¸‰ç§è®¡ç®—/ç»Ÿè®¡èµ„æºä¹‹ä¸€çš„ç“¶é¢ˆï¼šè®­ç»ƒæ—¶é—´ $t$ï¼Œæ¨¡å‹å¤§å° $N$ å’Œå¯ç”¨æ•°æ®æ€»é‡ $P$ã€‚ è¿™æ„å‘³ç€å³ä½¿å…¶ä»–ä¸¤ä¸ªèµ„æºå®é™…ä¸Šæ˜¯æ— é™çš„ï¼ŒæŸå¤±ä»ç„¶å¯èƒ½æ˜¯éé›¶çš„ï¼Œå› ä¸ºç¬¬ä¸‰ä¸ªé‡çš„æœ‰é™å€¼ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æŸå¤±å¯¹è¿™äº›èµ„æºçš„ä¾èµ–å…³ç³»åœ¨ç‰¹å¾æœ¬èº«å…·æœ‰å¹‚å¾‹ç»“æ„æ—¶å¯ä»¥éµå¾ªå¹‚å¾‹ã€‚å·²ç»è§‚å¯Ÿåˆ°ï¼ŒçœŸå®æ•°æ®é›†ä¸Šç¥ç»ç½‘ç»œæ ¸çš„è°±é€šå¸¸éµå¾ªå¹‚å¾‹ã€‚  
$$\lambda_k = \mathbb{E}_{x} \psi_k(x)^2 \sim k^{-b} \ , \ [\mathbb{E}_x y(x) \psi_k(x) ]^2 \sim k^{-a} .$$

For this kind of feature structure, our theory gives the following approximate scaling laws when bottlenecked by one of the three resources (time, model size, and dataset size)  
å¯¹äºè¿™ç§ç‰¹å¾ç»“æ„ï¼Œæˆ‘ä»¬çš„ç†è®ºåœ¨å—åˆ°ä¸‰ç§èµ„æºï¼ˆæ—¶é—´ã€æ¨¡å‹å¤§å°å’Œæ•°æ®é›†å¤§å°ï¼‰ä¹‹ä¸€çš„ç“¶é¢ˆæ—¶ï¼Œç»™å‡ºäº†ä»¥ä¸‹è¿‘ä¼¼ç¼©æ”¾æ³•åˆ™

\begin{align}  
\mathcal L(t,P,N) \approx  
\begin{cases}  
t^{-(a-1)/b} \ , \ N,P \to \infty  
\\  
N^{-\min\{a-1,2b\}} \ , \ t,P \to \infty  
\\  
P^{-\min\{a-1,2b\}} \ , \ t, N \to \infty  
\end{cases}  
\end{align}

For most cases of interest, the expoents satisfy $a-1 < 2b$[14,17](#references), leading to $\sim N^{-(a-1)}, P^{-(a-1)}$ model and data bottleneck scaling laws. In these cases, our result predicts that in general the training time exponent is smaller than model size or data exponents, depending on the rate of decay of the eigenvalues, set by $b$.  
å¯¹äºå¤§å¤šæ•°æ„Ÿå…´è¶£çš„æƒ…å†µï¼ŒæŒ‡æ•°æ»¡è¶³ $a-1 < 2b$[14,17](#references)ï¼Œå¯¼è‡´ $\sim N^{-(a-1)}, P^{-(a-1)}$ æ¨¡å‹å’Œæ•°æ®ç“¶é¢ˆç¼©æ”¾å®šå¾‹ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ç»“æœé¢„æµ‹ä¸€èˆ¬è®­ç»ƒæ—¶é—´æŒ‡æ•°å°äºæ¨¡å‹å¤§å°æˆ–æ•°æ®æŒ‡æ•°ï¼Œè¿™å–å†³äºç”± $b$ è®¾ç½®çš„ç‰¹å¾å€¼çš„è¡°å‡é€Ÿç‡ã€‚

An intuitive way to interpret this result in the case of interest ($\min{a-1,2b} = a-1$) is that $t$ steps of gradient descent on $N$ features and $P$ data can capture at most  
ä¸€ç§ç›´è§‚çš„æ–¹æ³•æ¥è§£é‡Šè¿™ä¸€ç»“æœï¼ˆåœ¨å…³æ³¨çš„æƒ…å†µä¸‹ï¼Œ$\min{a-1,2b} = a-1$ï¼‰æ˜¯ï¼Œ$t$æ­¥çš„æ¢¯åº¦ä¸‹é™åœ¨$N$ä¸ªç‰¹å¾å’Œ$P$ä¸ªæ•°æ®ä¸Šæœ€å¤šå¯ä»¥æ•è·  
$$k_{\star} \approx \min{ t^{1/b}, N, P } . $$  
spectral components of the target function. The loss is determined by the remaining variance that is not captured in these top $k_\star$ components $\mathcal L \approx \sum_{k > k_\star} \mathbb{E}_{x}[y(x) \psi_k(x)]^2$. Thus these bottleneck scaling laws can be viewed low-rank effects in the empirical kernel that limit the performance of the model.  
ç›®æ ‡å‡½æ•°çš„é¢‘è°±æˆåˆ†ã€‚æŸå¤±ç”±æœªåŒ…å«åœ¨è¿™ $k_\star$ ä¸ªæˆåˆ†ä¸­çš„å‰©ä½™æ–¹å·®å†³å®š $\mathcal L \approx \sum_{k > k_\star} \mathbb{E}_{x}[y(x) \psi_k(x)]^2$ã€‚å› æ­¤ï¼Œè¿™äº›ç“¶é¢ˆç¼©æ”¾æ³•åˆ™å¯ä»¥è¢«è§†ä¸ºé™åˆ¶æ¨¡å‹æ€§èƒ½çš„ç»éªŒæ ¸ä¸­çš„ä½ç§©æ•ˆåº”ã€‚

Compute Optimal Scaling Laws for this Model  
è®¡ç®—æ­¤æ¨¡å‹çš„æœ€ä¼˜ç¼©æ”¾æ³•åˆ™
----------------------------------------------------------

In this section we consider a regime of training where there is sufficient data, such as the online training regime of large language models. By approximating the test loss as a linear combination of the model size and time bottleneck scalings, we can derive the compute optimal scaling of training time and model size with respect to total compute $C=N t$. This compute budget $C$ is the total number of floating point operations required to train the model. For the optimal choice of training time and model size, we find the loss depends on compute as  
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªè®­ç»ƒæ¨¡å¼ï¼Œå…¶ä¸­æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œä¾‹å¦‚å¤§è¯­è¨€æ¨¡å‹çš„åœ¨çº¿è®­ç»ƒæ¨¡å¼ã€‚é€šè¿‡å°†æµ‹è¯•æŸå¤±è¿‘ä¼¼ä¸ºæ¨¡å‹è§„æ¨¡å’Œæ—¶é—´ç“¶é¢ˆç¼©æ”¾çš„çº¿æ€§ç»„åˆï¼Œæˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡ºç›¸å¯¹äºæ€»è®¡ç®— $C=N t$ çš„è®­ç»ƒæ—¶é—´å’Œæ¨¡å‹è§„æ¨¡çš„è®¡ç®—æœ€ä¼˜ç¼©æ”¾ã€‚è¿™ä¸ªè®¡ç®—é¢„ç®— $C$ æ˜¯è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„æµ®ç‚¹è¿ç®—æ€»æ•°ã€‚å¯¹äºè®­ç»ƒæ—¶é—´å’Œæ¨¡å‹è§„æ¨¡çš„æœ€ä¼˜é€‰æ‹©ï¼Œæˆ‘ä»¬å‘ç°æŸå¤±ä¾èµ–äºè®¡ç®—ï¼Œå› ä¸º

$$\mathcal L_\star(C) \sim C^{-\min{a-1,2b}(a-1) /( b \min{a-1,2b} + a-1)}$$

which in most cases of interest will simply be $\mathcal{L}_\star(C) \sim C^{- (a-1)/(b+1)}$. We show an example of this for $(a,b) = (2,1)$ below. Our theoretical scaling law is compared to the experimental loss curves from training models of varying size $N$ for multiple timesteps.  
åœ¨å¤§å¤šæ•°æ„Ÿå…´è¶£çš„æƒ…å†µä¸‹ï¼Œå°†ç®€å•åœ°è¡¨ç¤ºä¸º $\mathcal{L}_\star(C) \sim C^{- (a-1)/(b+1)}$ã€‚æˆ‘ä»¬åœ¨ä¸‹é¢å±•ç¤ºäº† $(a,b) = (2,1)$ çš„ç¤ºä¾‹ã€‚æˆ‘ä»¬çš„ç†è®ºç¼©æ”¾å®šå¾‹ä¸é’ˆå¯¹å¤šä¸ªæ—¶é—´æ­¥é•¿è®­ç»ƒä¸åŒå¤§å° $N$ çš„æ¨¡å‹çš„å®éªŒæŸå¤±æ›²çº¿è¿›è¡Œäº†æ¯”è¾ƒã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.34.51-AM-1024x775.png)

This model shows how the data structure and architecture influence the compute costs of training a highly performant model. Specifically, the decay rate of target coefficients and eigenvalues controls the compute optimal scaling law of the model. For models with fast eigenvalue decay rates, it is preferable to scale up training time much faster than scaling up model size as the optimal scaling rule is $t \sim C^{\frac{b}{1+b}}$ and $N \sim C^{\frac{1}{1+b}}$. As $b \to 1$ the optimal scaling is symmetric.  
è¯¥æ¨¡å‹å±•ç¤ºäº†æ•°æ®ç»“æ„å’Œæ¶æ„å¦‚ä½•å½±å“è®­ç»ƒé«˜æ€§èƒ½æ¨¡å‹çš„è®¡ç®—æˆæœ¬ã€‚å…·ä½“è€Œè¨€ï¼Œç›®æ ‡ç³»æ•°å’Œç‰¹å¾å€¼çš„è¡°å‡ç‡æ§åˆ¶æ¨¡å‹çš„è®¡ç®—æœ€ä½³æ‰©å±•è§„å¾‹ã€‚å¯¹äºç‰¹å¾å€¼è¡°å‡ç‡è¾ƒå¿«çš„æ¨¡å‹ï¼Œæ›´åŠ å¸Œæœ›å°†è®­ç»ƒæ—¶é—´çš„æ‰©å±•é€Ÿåº¦è¿œå¿«äºæ¨¡å‹è§„æ¨¡çš„æ‰©å±•ï¼Œå› ä¸ºæœ€ä½³æ‰©å±•è§„åˆ™æ˜¯ $t \sim C^{\frac{b}{1+b}}$ å’Œ $N \sim C^{\frac{1}{1+b}}$ã€‚å½“ $b \to 1$ æ—¶ï¼Œæœ€ä½³æ‰©å±•æ˜¯å¯¹ç§°çš„ã€‚

Build up of Finite Width Effects  
æœ‰é™å®½åº¦æ•ˆåº”çš„ç§¯ç´¯
--------------------------------------------

Many works have observed that the early training-time dynamics of networks with width $N$ deviates from the infinite $N$ limit with a scaling rate of $1/N$[12-14](#references), but that after a long amount of training on sufficient quantities of data the convergence rate exhibits a task-dependent scaling law $N^{-\alpha_N}$[13-14](#references). Our model also exhibits a transition in the convergence rates as training takes place. Below we show the early time loss of our model at $N$ compared to our model in the $N \to \infty$ limit, seeing a $1/N$ convergence rate.  
è®¸å¤šç ”ç©¶è§‚å¯Ÿåˆ°ï¼Œå®½åº¦ä¸º $N$ çš„ç½‘ç»œåœ¨æ—©æœŸè®­ç»ƒæ—¶é—´çš„åŠ¨æ€è¡Œä¸ºåç¦»æ— ç©·å¤§ $N$ æé™ï¼Œå…¶ç¼©æ”¾ç‡ä¸º $1/N$[12-14](#references)ï¼Œä½†åœ¨ç»è¿‡å¤§é‡æ•°æ®çš„é•¿æ—¶é—´è®­ç»ƒåï¼Œæ”¶æ•›é€Ÿç‡è¡¨ç°å‡ºä¾èµ–äºä»»åŠ¡çš„ç¼©æ”¾å¾‹ $N^{-\alpha_N}$[13-14](#references)ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¹Ÿæ˜¾ç¤ºå‡ºæ”¶æ•›é€Ÿç‡çš„è½¬å˜ã€‚ä»¥ä¸‹æˆ‘ä»¬å±•ç¤ºäº†åœ¨ $N$ æ—¶æˆ‘ä»¬æ¨¡å‹çš„æ—©æœŸæ—¶é—´æŸå¤±ï¼Œä¸åœ¨ $N \to \infty$ æé™ä¸‹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œè§‚å¯Ÿåˆ° $1/N$ æ”¶æ•›é€Ÿç‡ã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.38.20-AM-1024x754.png)

However, after significant training time, the model will eventually depend on the model size $N$ with a scaling exponent that is task-dependent (the bottleneck scaling) as we show below.  
ç„¶è€Œï¼Œåœ¨ç»è¿‡å¤§é‡è®­ç»ƒæ—¶é—´åï¼Œè¯¥æ¨¡å‹æœ€ç»ˆå°†ä¾èµ–äºæ¨¡å‹å¤§å° $N$ï¼Œå…¶ç¼©æ”¾æŒ‡æ•°å–å†³äºä»»åŠ¡ï¼ˆç“¶é¢ˆç¼©æ”¾ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.38.30-AM-1024x757.png)

We see that this scaling law can significantly differ from the $1/N$ rate and indeed becomes task dependent.  
æˆ‘ä»¬çœ‹åˆ°è¿™ä¸€ç¼©æ”¾æ³•åˆ™ä¸ $1/N$ é€Ÿç‡å¯ä»¥æ˜¾è‘—ä¸åŒï¼Œå®é™…ä¸Šå®ƒå˜å¾—ä¾èµ–äºä»»åŠ¡ã€‚

Data Reuse and Buildup of Test/Train Loss Gaps  
æ•°æ®é‡ç”¨å’Œæµ‹è¯•/è®­ç»ƒæŸå¤±å·®è·çš„ç§¯ç´¯
------------------------------------------------------------------

Many works have also observed that the early time training with a finite dataset is well approximated by training with infinite data[10-11](#references), however over time a gap develops between training and test losses. This is also a naturally occuring feature in our model and the DMFT equations exactly describe how the test and train losses diverge over time. Below we plot dynamics for $N=512$ with varying dataset size $P$.  
è®¸å¤šç ”ç©¶è¿˜è§‚å¯Ÿåˆ°ï¼Œä½¿ç”¨æœ‰é™æ•°æ®é›†çš„æ—©æœŸè®­ç»ƒä¸æ— é™æ•°æ®è®­ç»ƒçš„ç»“æœéå¸¸æ¥è¿‘[10-11](#references)ï¼Œç„¶è€Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œè®­ç»ƒæŸå¤±å’Œæµ‹è¯•æŸå¤±ä¹‹é—´å‡ºç°äº†å·®è·ã€‚è¿™ä¹Ÿæ˜¯æˆ‘ä»¬æ¨¡å‹ä¸­è‡ªç„¶å‘ç”Ÿçš„ç‰¹å¾ï¼ŒDMFT æ–¹ç¨‹å‡†ç¡®æè¿°äº†æµ‹è¯•æŸå¤±å’Œè®­ç»ƒæŸå¤±éšæ—¶é—´å¦‚ä½•å‘æ•£ã€‚ä¸‹é¢æˆ‘ä»¬ç»˜åˆ¶äº† $N=512$ åœ¨ä¸åŒæ•°æ®é›†å¤§å° $P$ ä¸‹çš„åŠ¨æ€ã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.41.58-AM-1024x758.png)

We note that the test and train losses are close initially but accumulate finite $P$ corrections that drive the separation of test and train. These corrections are larger for small $P$ and vanish as $P \to \infty$.  
æˆ‘ä»¬æ³¨æ„åˆ°æµ‹è¯•å’Œè®­ç»ƒæŸå¤±æœ€åˆæ¥è¿‘ï¼Œä½†ç§¯ç´¯æœ‰é™çš„ $P$ ä¿®æ­£ï¼Œå¯¼è‡´æµ‹è¯•å’Œè®­ç»ƒçš„åˆ†ç¦»ã€‚å¯¹äºå° $P$ï¼Œè¿™äº›ä¿®æ­£æ›´å¤§ï¼Œè€Œå½“ $P \to \infty$ æ—¶æ¶ˆå¤±ã€‚

Ensembling Often Outperformed by Increasing Width  
é›†æˆé€šå¸¸é€šè¿‡å¢åŠ å®½åº¦è·å¾—æ›´å¥½çš„è¡¨ç°
---------------------------------------------------------------------

Finite sized models with random initial weights can be thought of noisy approximations of infinitely sized neural networks. This extra noise can lead to worse performance and can be eliminated by training multiple models with independent initialization in parallel and averaging their outputs, a procedure known as ensembling. However recent experiments have demonstrated that the benefits to ensembling, while non-negligible, are not as significant as the benefit of increasing model size[12-13](#references).  
å…·æœ‰éšæœºåˆå§‹æƒé‡çš„æœ‰é™å¤§å°æ¨¡å‹å¯ä»¥è¢«è§†ä¸ºæ— é™å¤§å°ç¥ç»ç½‘ç»œçš„å˜ˆæ‚è¿‘ä¼¼ã€‚é¢å¤–çš„å™ªå£°å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œé€šè¿‡å¹¶è¡Œè®­ç»ƒå¤šä¸ªç‹¬ç«‹åˆå§‹åŒ–çš„æ¨¡å‹å¹¶å°†å…¶è¾“å‡ºå¹³å‡ï¼Œå¯ä»¥æ¶ˆé™¤è¿™ç§å™ªå£°ï¼Œè¿™ä¸€è¿‡ç¨‹ç§°ä¸ºé›†æˆã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡é›†æˆçš„å¥½å¤„ä¸å®¹å¿½è§†ï¼Œä½†å…¶å¥½å¤„å¹¶ä¸å¦‚å¢åŠ æ¨¡å‹å¤§å°çš„å¥½å¤„æ˜¾è‘—[12-13](#references)ã€‚

In our toy model, we can analyze the effect of ensembling on the test loss and ask whether ensembling is compute optimal. Training an ensemble of $E$ networks and averaging their outputs would incur a compute cost of $C = E N t$. Below we plot loss as a function of compute for $E=1$ and $E=4$ ensembles for varying width $N$.  
åœ¨æˆ‘ä»¬çš„ç©å…·æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†æé›†æˆå¯¹æµ‹è¯•æŸå¤±çš„å½±å“ï¼Œå¹¶è¯¢é—®é›†æˆæ˜¯å¦æ˜¯è®¡ç®—æœ€ä¼˜çš„ã€‚è®­ç»ƒä¸€ä¸ªç”± $E$ ä¸ªç½‘ç»œç»„æˆçš„é›†æˆå¹¶å¹³å‡å®ƒä»¬çš„è¾“å‡ºå°†äº§ç”Ÿè®¡ç®—æˆæœ¬ $C = E N t$ã€‚ä¸‹é¢æˆ‘ä»¬å°†æŸå¤±ç»˜åˆ¶ä¸ºè®¡ç®—çš„å‡½æ•°ï¼Œé’ˆå¯¹ $E=1$ å’Œ $E=4$ çš„é›†æˆï¼Œå¹¶æ”¹å˜å®½åº¦ $N$ã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.44.17-AM-1024x768.png)

At each value of compute $C$, it is prefereable to choose the larger model with $E=1$ than to use a smaller model with $E=4$. We argue the reason for this is that doubling $N$ has a similar effect on the variance as doubling $E$. However, doubling $N$ also reduces the _bias_.  
åœ¨æ¯ä¸ªè®¡ç®—å€¼$C$ä¸‹ï¼Œé€‰æ‹©$E=1$çš„è¾ƒå¤§æ¨¡å‹æ¯”ä½¿ç”¨$E=4$çš„å°æ¨¡å‹æ›´å¯å–ã€‚æˆ‘ä»¬è®¤ä¸ºåŸå› åœ¨äºï¼Œç¿»å€$N$å¯¹æ–¹å·®çš„å½±å“ä¸ç¿»å€$E$ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œç¿»å€$N$ä¹Ÿå‡å°‘äº†_åå·®_ã€‚

Bias and Mode ErrorsÂ åå·®å’Œæ¨¡å¼é”™è¯¯
----------------------------

To give a flavor of how this theory works, we show how DMFT recovers the bias along the $k$th feature for all $k$. This error is given by: $$ H_k(t) = \frac{\mathbb{E}_{x} [(y(x) â€“ f(x,t)) \psi_k(x)] }{\mathbb{E}_{x} [y(x) \psi_k(x)]}$$  
ä¸ºäº†è®©æ‚¨äº†è§£è¿™ä¸ªç†è®ºæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæˆ‘ä»¬å±•ç¤ºäº† DMFT å¦‚ä½•æ¢å¤æ‰€æœ‰ $k$ çš„ç¬¬ $k$ ä¸ªç‰¹å¾çš„åå·®ã€‚è¿™ä¸ªè¯¯å·®ç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š$$ H_k(t) = \frac{\mathbb{E}_{x} [(y(x) â€“ f(x,t)) \psi_k(x)] }{\mathbb{E}_{x} [y(x) \psi_k(x)]}$$

Our theory explictly calculates the Fourier transform $\mathcal H_k(\omega)$ in closed form. This is given in terms of the eigenvalues $\lambda_k$, the dataset size $P$ and the model size $N$. An example of the closed form solution for the $H_k(t)$ is plotted below with $N = 128$ and varying values for $P$.  
æˆ‘ä»¬çš„ç†è®ºæ˜ç¡®åœ°ä»¥å°é—­å½¢å¼è®¡ç®—å‚…é‡Œå¶å˜æ¢ $\mathcal H_k(\omega)$ã€‚è¿™æ˜¯ç”¨ç‰¹å¾å€¼ $\lambda_k$ã€æ•°æ®é›†å¤§å° $P$ å’Œæ¨¡å‹å¤§å° $N$ è¡¨ç¤ºçš„ã€‚ä¸‹é¢ç»˜åˆ¶äº† $H_k(t)$ çš„å°é—­å½¢å¼è§£çš„ç¤ºä¾‹ï¼Œ$N = 128$ï¼Œå¹¶ä¸” $P$ çš„å€¼å„ä¸ç›¸åŒã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.45.48-AM-1024x618.png)

The error along the $k$-th eigendirection deviates from the infinite data and infinite model limit (gray lines) and eventually saturates as $t \to \infty$, giving a final loss which depends on $N$ and $P$. Even if $P \to \infty$, the $H_k$ curves saturate in this plot due to the finite value of $N = 128$. We show the losses for $k=1$ (solid) and $k=10$ (dashed). We find that the bias, which is set by $H_k$ decreases as $N,P$ and $t$ increase.  
æ²¿ç€ç¬¬ $k$ ä¸ªç‰¹å¾æ–¹å‘çš„è¯¯å·®åç¦»äº†æ— é™æ•°æ®å’Œæ— é™æ¨¡å‹æé™ï¼ˆç°çº¿ï¼‰ï¼Œå¹¶æœ€ç»ˆåœ¨ $t \to \infty$ æ—¶é¥±å’Œï¼Œäº§ç”Ÿçš„æœ€ç»ˆæŸå¤±ä¾èµ–äº $N$ å’Œ $P$ã€‚å³ä½¿ $P \to \infty$ï¼Œç”±äº $N = 128$ çš„æœ‰é™å€¼ï¼Œ$H_k$ æ›²çº¿åœ¨æ­¤å›¾ä¸­ä¹Ÿé¥±å’Œã€‚æˆ‘ä»¬å±•ç¤ºäº† $k=1$ï¼ˆå®çº¿ï¼‰å’Œ $k=10$ï¼ˆè™šçº¿ï¼‰çš„æŸå¤±ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”± $H_k$ è®¾ç½®çš„åå·®éšç€ $N$ã€$P$ å’Œ $t$ çš„å¢åŠ è€Œå‡å°ã€‚

One Pass SGD and Batch Noise  
ä¸€è½®éšæœºæ¢¯åº¦ä¸‹é™å’Œæ‰¹é‡å™ªå£°
--------------------------------------------

We can also use our methods to analyze stochastic gradient descent (SGD) in discrete time without data reuse. In this setting, the finite model size and finite training time can still limit performance, but the finite batch size $B$ only introduces additional _variance_ in the dynamics as we illustrate below. On the left, we vary the model size $N$ with batchsize set to $B=32$ and see that we still obtain model size bottlenecks which are qualitatively similar to before. We also see additional small fluctuations in the loss from batch to batch. On the right, we show $N=256$ with varying batch size, showing that the expected loss and scale of fluctuations are higher for smaller batches.  
æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¦»æ•£æ—¶é—´å†…åˆ†æéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼Œè€Œä¸é‡ç”¨æ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ‰é™çš„æ¨¡å‹å¤§å°å’Œæœ‰é™çš„è®­ç»ƒæ—¶é—´ä»ç„¶ä¼šé™åˆ¶æ€§èƒ½ï¼Œä½†æœ‰é™çš„æ‰¹é‡å¤§å° $B$ ä»…ä¼šåœ¨åŠ¨æ€ä¸­å¼•å…¥é¢å¤–çš„ _æ–¹å·®_ï¼Œå¦‚ä¸‹é¢æ‰€ç¤ºã€‚åœ¨å·¦ä¾§ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å¤§å° $N$ å˜åŒ–ï¼Œæ‰¹é‡å¤§å°è®¾ç½®ä¸º $B=32$ï¼Œå¯ä»¥çœ‹åˆ°æˆ‘ä»¬ä»ç„¶è·å¾—äº†æ¨¡å‹å¤§å°çš„ç“¶é¢ˆï¼Œè¿™ä¸ä¹‹å‰çš„æƒ…å†µ qualitatively ç±»ä¼¼ã€‚æˆ‘ä»¬è¿˜çœ‹åˆ°æŸå¤±åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¹‹é—´æœ‰é¢å¤–çš„å°æ³¢åŠ¨ã€‚åœ¨å³ä¾§ï¼Œæˆ‘ä»¬å±•ç¤ºäº† $N=256$ï¼Œæ‰¹é‡å¤§å°ä¸åŒï¼Œæ˜¾ç¤ºå‡ºè¾ƒå°çš„æ‰¹é‡çš„é¢„æœŸæŸå¤±å’Œæ³¢åŠ¨è§„æ¨¡æ›´é«˜ã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.47.38-AM-1024x379.png)

In this setting, a test-train gap is not possible since every fresh batch of data gives an unbiased estimate of the population loss. As a consequence, online learning does not experience a data bottleneck in the bias, but only additional variance from the fluctuations in the SGD updates. These updates disappear in the continuous time (infinitesimal learning rate) limit which recovers the infinite data $P \to \infty$ limit of the previously discussed gradient flow equations.  
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæµ‹è¯•-è®­ç»ƒå·®è·æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºæ¯ä¸€æ‰¹æ–°æ•°æ®éƒ½èƒ½æä¾›å¯¹æ€»ä½“æŸå¤±çš„æ— åä¼°è®¡ã€‚å› æ­¤ï¼Œåœ¨çº¿å­¦ä¹ ä¸ä¼šåœ¨åå·®ä¸Šå‡ºç°æ•°æ®ç“¶é¢ˆï¼Œè€Œåªæ˜¯ç”±äº SGD æ›´æ–°ä¸­çš„æ³¢åŠ¨è€Œäº§ç”Ÿé¢å¤–çš„æ–¹å·®ã€‚è¿™äº›æ›´æ–°åœ¨è¿ç»­æ—¶é—´ï¼ˆæ— ç©·å°å­¦ä¹ ç‡ï¼‰é™åˆ¶ä¸­æ¶ˆå¤±ï¼Œä»è€Œæ¢å¤äº†ä¹‹å‰è®¨è®ºçš„æ¢¯åº¦æµæ–¹ç¨‹çš„æ— é™æ•°æ® $P \to \infty$ é™åˆ¶ã€‚

What is Missing? Feature Learning Scaling Laws  
ç¼ºå¤±çš„æ˜¯ä»€ä¹ˆï¼Ÿç‰¹å¾å­¦ä¹ çš„æ‰©å±•å®šå¾‹
-----------------------------------------------------------------

Our model is based on a kernel approximation of neural network training which fails to capture the benefits to performance due to feature learning. Below we plot neural networks trained in the kernel regime (solid) and the predicted compute scaling exponent (blue), obtained from fitting the exponents $a$ and $b$ to the measured initial kernel spectra. We also plot the loss curves for networks in the feature learning regime (dotted lines).  
æˆ‘ä»¬çš„æ¨¡å‹åŸºäºç¥ç»ç½‘ç»œè®­ç»ƒçš„æ ¸é€¼è¿‘ï¼Œä½†æœªèƒ½æ•æ‰åˆ°ç‰¹å¾å­¦ä¹ å¸¦æ¥çš„æ€§èƒ½æå‡ã€‚ä¸‹é¢æˆ‘ä»¬ç»˜åˆ¶äº†åœ¨æ ¸é¢†åŸŸè®­ç»ƒçš„ç¥ç»ç½‘ç»œï¼ˆå®çº¿ï¼‰å’Œé€šè¿‡æ‹Ÿåˆæµ‹å¾—çš„åˆå§‹æ ¸è°±å¾—åˆ°çš„é¢„æµ‹è®¡ç®—ç¼©æ”¾æŒ‡æ•°ï¼ˆè“è‰²ï¼‰ï¼Œè¯¥æŒ‡æ•°ä¸º$a$å’Œ$b$ã€‚æˆ‘ä»¬è¿˜ç»˜åˆ¶äº†ç‰¹å¾å­¦ä¹ é¢†åŸŸç½‘ç»œçš„æŸå¤±æ›²çº¿ï¼ˆè™šçº¿ï¼‰ã€‚

![](https://kempnerinstitute.harvard.edu/app/uploads/2024/06/Screenshot-2024-06-11-at-9.47.46-AM-1024x711.png)

While the networks operating in the kernel regime (solid) are well described by our theoretical prediction for the compute scaling law, the networks in the rich, feature learning regime have a much better dependence on compute $C$. This illustrates that quantitatively capturing the compute optimal scaling exponents observed in practice will require a theory of how feature learning accelerates convergence during training.  
è™½ç„¶åœ¨å†…æ ¸æœºåˆ¶ï¼ˆå›ºæ€ï¼‰ä¸‹è¿è¡Œçš„ç½‘ç»œå¾ˆå¥½åœ°ç¬¦åˆæˆ‘ä»¬å¯¹è®¡ç®—è§„æ¨¡æ³•åˆ™çš„ç†è®ºé¢„æµ‹ï¼Œä½†åœ¨ä¸°å¯Œçš„ç‰¹å¾å­¦ä¹ æœºåˆ¶ä¸‹çš„ç½‘ç»œå¯¹è®¡ç®— $C$ æœ‰æ›´å¥½çš„ä¾èµ–æ€§ã€‚è¿™è¡¨æ˜ï¼Œè¦å®šé‡æ•æ‰å®è·µä¸­è§‚å¯Ÿåˆ°çš„è®¡ç®—æœ€ä¼˜ç¼©æ”¾æŒ‡æ•°ï¼Œå°†éœ€è¦ä¸€ç§ç†è®ºæ¥è§£é‡Šç‰¹å¾å­¦ä¹ å¦‚ä½•åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ”¶æ•›ã€‚

ConclusionÂ ç»“è®º
-------------

We proposed a simple linear model to analyze dynamical neural scaling laws. This model captures many of the observed phenomena related to network training and test loss dynamics. Looking forward, theories which incorporate feature learning into the network training dynamics will improve our understanding of scaling laws. The fact that infinite sized models perform the best suggests that starting with theories of feature learning at infinite width are a good place to start.  
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹æ¥åˆ†æåŠ¨æ€ç¥ç»ç¼©æ”¾è§„å¾‹ã€‚è¯¥æ¨¡å‹æ•æ‰äº†ä¸ç½‘ç»œè®­ç»ƒå’Œæµ‹è¯•æŸå¤±åŠ¨æ€ç›¸å…³çš„è®¸å¤šè§‚å¯Ÿç°è±¡ã€‚å±•æœ›æœªæ¥ï¼Œå°†ç‰¹å¾å­¦ä¹ çº³å…¥ç½‘ç»œè®­ç»ƒåŠ¨æ€çš„ç†è®ºå°†æœ‰åŠ©äºæˆ‘ä»¬æ›´å¥½åœ°ç†è§£ç¼©æ”¾è§„å¾‹ã€‚æ— é™å¤§å°çš„æ¨¡å‹è¡¨ç°æœ€ä½³è¿™ä¸€äº‹å®è¡¨æ˜ï¼Œä»æ— é™å®½åº¦çš„ç‰¹å¾å­¦ä¹ ç†è®ºå¼€å§‹æ˜¯ä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ã€‚

ReferencesÂ å‚è€ƒæ–‡çŒ®
---------------

1.  Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.[[Return to text â†©](#identifier_0_8573)]  
    è´¾é‡Œå¾·Â·å¡æ™®å…°ï¼Œè¨å§†Â·éº¦è‚¯å¾·åˆ©ä»€ï¼Œæ±¤å§†Â·èµ«å°¼æ ¹ï¼Œæ±¤å§†Â·BÂ·å¸ƒæœ—ï¼Œæœ¬æ°æ˜Â·åˆ‡æ–¯ï¼Œç‘æ¸©Â·æŸ¥å°”å¾·ï¼Œæ–¯ç§‘ç‰¹Â·æ ¼é›·ï¼Œäºšå†å…‹Â·æ‹‰å¾·ç¦å¾·ï¼Œæ°å¼—é‡ŒÂ·Wuï¼Œä»¥åŠè¾¾é‡Œå¥¥Â·é˜¿è«ä»£ã€‚ç¥ç»è¯­è¨€æ¨¡å‹çš„è§„æ¨¡æ³•åˆ™ã€‚arXiv é¢„å°æœ¬ arXiv:2001.08361ï¼Œ2020ã€‚[[è¿”å›æ–‡æœ¬ â†©](#identifier_0_8573)]
2.  Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020.[[Return to text â†©](#identifier_1_8573)]  
    æ±¤å§†Â·å¸ƒæœ—ï¼Œæœ¬æ°æ˜Â·æ›¼ï¼Œå°¼å…‹Â·è±å¾·ï¼Œæ¢…æ‹‰å¦®Â·è¨æ¯”äºšï¼Œè´¾é‡Œå¾·Â·DÂ·å¡æ™®å…°ï¼Œæ™®æ‹‰æ³•æ‹‰Â·è¾¾é‡Œç“¦å°”ï¼Œé˜¿å°”æ–‡Â·å°¼æ‹‰åå¦ï¼Œæ™®æ‹‰çº³å¤«Â·å¤å§†ï¼Œå‰é‡Œä»€Â·è¨æ–¯ç‰¹é‡Œï¼Œé˜¿æ›¼è¾¾Â·é˜¿æ–¯å…‹å°”ç­‰äººã€‚è¯­è¨€æ¨¡å‹æ˜¯å°‘é‡æ ·æœ¬å­¦ä¹ è€…ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹ï¼Œ33:1877â€“1901ï¼Œ2020 å¹´ã€‚[[è¿”å›æ–‡æœ¬ â†©](#identifier_1_8573)]
3.  Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.[[Return to text â†©](#identifier_2_8573)]
4.  Gemini Team Google. Gemini: A Family of Highly Capable Multimodal Models, arXiv preprint arXiv:2203.15556, 2024.[[Return to text â†©](#identifier_3_8573)]
5.  Tamay Besiroglu, Ege Erdil, Matthew Barnett and Josh You. Chinchilla Scaling: A replication attempt. arXiv preprint arXiv:2404.10102, 2024.[[Return to text â†©](#identifier_4_8573)]
6.  Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.[[Return to text â†©](#identifier_5_8573)]
7.  Blake Bordelon, Alex Atanasov, Cengiz Pehlevan. Dynamical Model of Neural Scaling Laws. To appear International Conference of Machine Learning 2024.[[Return to text â†©](#identifier_6_8573)]
8.  Elliot Paquette,Â Courtney Paquette,Â Lechao Xiao,Â Jeffrey Pennington. 4+3 Phases of Compute-Optimal Neural Sclaing Laws. arXiv preprint arXiv:2405.15074, 2024.[[Return to text â†©](#identifier_7_8573)]
9.  Licong Lin,Â Jingfeng Wu,Â Sham M. Kakade, Peter L. Bartlett, Jason D. Lee. Scaling Laws in Linear Regression: Compute, Parameters, and Data. arXiv preprint arXiv:2406:08466, 2024.[[Return to text â†©](#identifier_8_8573)]
10.  Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good online learners are good offline generalizers. International Conference on Learning Representations, 2021[[Return to text â†©](#identifier_9_8573)]
11.  Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing systems, 2023[[Return to text â†©](#identifier_10_8573)]
12.  Blake Bordelon and Cengiz Pehlevan. Dynamics of finite width kernel and prediction fluctuations in mean field neural networks. Advances in Neural Information Processing Systems, 2023.[[Return to text â†©](#identifier_11_8573)]
13.  Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan. Feature-learning networks are consistent across widths at realistic scales, Advances in Neural Information Processing Systems 2023.[[Return to text â†©](#identifier_12_8573)]
14.  Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. arXiv preprint arXiv:2102.06701, 2021.[[Return to text â†©](#identifier_13_8573)]
15.  Alexander Maloney, Daniel Roberts, James Sully. A solvable model of neural scaling laws. arXiv preprint arXiv:2210.16859. 2022.[[Return to text â†©](#identifier_14_8573)]
16.  Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, Cengiz Pehlevan. Onset of Variance-limited Behavior for networks in the lazy and rich regimes. arXiv preprint arXiv:2212.12147. 2022.[[Return to text â†©](#identifier_15_8573)]
17.  Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In International Conference on Machine Learning, pp. 1024â€“1034. PMLR, 2020.[[Return to text â†©](#identifier_16_8573)]