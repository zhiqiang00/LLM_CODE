**2.5 长短期记忆网络 LSTM**

<font style="color:#3370FF;">1. </font>**LSTM介绍**

LSTM是一种时间循环网络（RNN），是为了解决RNN存在的长期依赖问题而专门设计出来的。

长期依赖问题：指的是RNN在处理序列状态的数据时，会遇到某个时间点的输出依赖于很久之前的输入或者这个时间点的隐状态时，就称这个问题是长期依赖问题。

具体来说，由于RNN共享同一个权重矩阵，在网络层次比较深，也就是序列长度过长的时候，每次参数的更新都需要这个权重矩阵进行乘法操作，在某个时刻，矩阵中某个值很小（或很大），那么随着层数的增加，梯度会指数级衰减直至为零（或者梯度会指数级增加，导致某些权重更新幅度过大，这会使得学习过程不稳定，模型可能会发散从而导致训练失败）。

LSTM之所以能够解决RNN的长期依赖问题，是因为LSTM引入了**门（gate）**机制用于控制特征的流通和损失。

<font style="color:#3370FF;">2. </font>**LSTM架构**

LSTM单元包括以下五个部分（有的文章内会说明有四个部分，这是将细胞态和隐状态结合了）：

**遗忘门（Forget Gate）**：决定哪些先前的记忆（即单元状态中的信息）需要被遗忘。使用sigmoid函数，生成一个指示哪些旧信息应被丢弃的向量。

                                                

a.  是遗忘门的输出，决定了上一个时间步细胞态中的哪些部分要遗忘。

b.  是sigmoid函数，它将输入映射到(0, 1)之间，代表每个元素保留的比例。

c. 遗忘门的权重矩阵。

d. 是一个拼接向量，包含上一时间步的隐藏状态和当前时间步的输入。

e. 是遗忘门的偏置项。

**输入门（Input Gate）**：

1.决定当前输入中哪些部分需要存储到细胞态（Cell State）中。它通过sigmoid函数生成一个介于0到1之间的向量，表示每个维度上新信息的保留程度。

                                      

表示输入门的输出，决定新信息中哪些部分应该被更新到细胞状态中。其余和遗忘门类似。

2.计算候选细胞状态

                                                 

      是候选值，代表希望添加到细胞状态的新信息，通过tanh函数进行压缩。                

**细胞态（Cell State）**：这是LSTM的记忆单元，可以看作是一种长期记忆存储器。它能够选择性地添加新的信息（通过输入门）和遗忘旧的信息（通过遗忘门）。

                                                 

       表示逐元素乘法

**输出门（Output Gate）**：决定哪些存储在单元状态中的信息应该被用于当前时间步的输出。    

                                                  

**隐状态（Hidden State）**：隐藏状态是基于当前细胞状态和输出门的输出：

                                                  

     将细胞状态转换为-1到1的范围，然后通过与输出门的逐元素乘法确定最终输出。

![](https://cdn.nlark.com/yuque/0/2024/png/1805392/1729130251878-d17c3c56-ee56-4d01-91b4-da34a221ba82.png)

<font style="color:#8F959E;">图源：</font>[<font style="color:#8F959E;">https://medium.com/@divyanshu132/lstm-and-its-equations-5ee9246d04af</font>](https://medium.com/@divyanshu132/lstm-and-its-equations-5ee9246d04af)

<font style="color:#3370FF;">3. </font>**使用LSTM进行简单的文本生成**

| <font style="color:rgb(100, 106, 115);">Python</font>import torch   from torch import nn   from torch.utils.data import Dataset, DataLoader   import random      _# 数据预处理_   **class** TextDataset(Dataset):       **def** __init__(self, text, seq_length):           self.chars = sorted(list(set(text)))           self.vocab_size = len(self.chars)           self.char_to_idx = {char: idx for idx, char in enumerate(self.chars)}           self.idx_to_char = {idx: char for idx, char in enumerate(self.chars)}           self.seq_length = seq_length           self.text_encoded = [self.char_to_idx[char] for char in text]           self.data = []                      for i in range(0, len(self.text_encoded) - seq_length):               self.data.append((torch.tensor(self.text_encoded[i:i+seq_length], dtype=torch.long),                                 torch.tensor(self.text_encoded[i+1:i+seq_length+1], dtype=torch.long)))                      **def** __len__(self):           return len(self.data)          **def** __getitem__(self, index):           return self.data[index]          **def** get_random_start(self):           return torch.tensor([self.char_to_idx[random.choice(self.chars)]], dtype=torch.long)      _# 定义LSTM模型_   **class** LSTMTextGenerator(nn.Module):       **def** __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):           super(LSTMTextGenerator, self).__init__()           self.hidden_dim = hidden_dim           self.embedding = nn.Embedding(vocab_size, embedding_dim)           self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)           self.fc = nn.Linear(hidden_dim, vocab_size)          **def** forward(self, x, hidden=None):           embeds = self.embedding(x)           lstm_out, hidden = self.lstm(embeds, hidden)           output = self.fc(lstm_out)           return output, hidden          **def** init_hidden(self, batch_size, device):           return (               torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(device),               torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(device)           )      _# 训练函数_   **def** train(model, dataloader, criterion, optimizer, device):       model.train()       total_loss = 0       for inputs, targets in dataloader:           inputs, targets = inputs.to(device), targets.to(device)           optimizer.zero_grad()           hidden = model.init_hidden(inputs.size(0), device)           outputs, hidden = model(inputs, hidden)           loss = criterion(outputs.reshape(-1, outputs.shape[-1]), targets.reshape(-1))           loss.backward()           optimizer.step()           total_loss += loss.item()       return total_loss / len(dataloader)      _# 文本生成函数_   **def** generate_text(model, start_token, num_generate, device, temperature=1.0):       model.eval()       hidden = model.init_hidden(1, device)  _# 初始化隐藏状态和细胞状态_       generated = start_token       with torch.no_grad():           for _ in range(num_generate):               input_tensor = generated[-1].unsqueeze(0).to(device)  _# 维度为 (1, embedding_dim)_               output, hidden = model(input_tensor, (hidden[0].squeeze(0), hidden[1].squeeze(0)))               output = output[:, -1, :].div(temperature).exp()               predicted_idx = torch.multinomial(output, 1)               generated = torch.cat((generated, predicted_idx), dim=1)       return generated.cpu().numpy().flatten()      _# 主程序_   if __name__ == "__main__":       text = "Hello, this is an example of text used for LSTM text generation. Let's generate some text!"       seq_length = 50       batch_size = 32       embedding_dim = 256       hidden_dim = 512       n_layers = 2       n_epochs = 100       device = torch.device("cuda" if torch.cuda.is_available() else "cpu")              dataset = TextDataset(text, seq_length)       data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)              model = LSTMTextGenerator(dataset.vocab_size, embedding_dim, hidden_dim, n_layers).to(device)       criterion = nn.CrossEntropyLoss()       optimizer = torch.optim.Adam(model.parameters(), lr=0.001)              for epoch in range(1, n_epochs + 1):           loss = train(model, data_loader, criterion, optimizer, device)           print(**f**"Epoch: {epoch}/{n_epochs}, Loss: {loss**:.4f**}")              start_token = dataset.get_random_start().unsqueeze(0).to(device)       generated_text = generate_text(model, start_token, num_generate=200, device=device)              _# 将索引转换回字符并打印生成的文本_       generated_chars = [dataset.idx_to_char[int(idx)] for idx in generated_text]       print(''.join(generated_chars)) |
| :--- |


结果：

![](https://cdn.nlark.com/yuque/0/2024/png/1805392/1729130252056-4e791e85-4fd5-4636-8374-9d3fcccf4af5.png)

**参考：**

[1]https://baike.baidu.com/item/%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/17541107?fr=ge_ala

[2]https://medium.com/@divyanshu132/lstm-and-its-equations-5ee9246d04af

