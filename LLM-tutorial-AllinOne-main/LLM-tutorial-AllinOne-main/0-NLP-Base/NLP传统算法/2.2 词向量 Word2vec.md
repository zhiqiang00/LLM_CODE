**2.2 词向量 Word2vec**

Word2Vec是一种用于将词语嵌入（word embedding）到低维向量空间的方法，它由Tomas Mikolov等人在2013年提出。Word2Vec的目标是通过神经网络模型，将词语表示成实值向量，使得具有相似语义的词语在向量空间中彼此接近。Word2Vec有两种主要的模型结构：**连续词袋模型（Continuous Bag of Words, CBOW）和跳字模型（Skip-gram）**。

<font style="color:#3370FF;">1. </font>**跳字模型（Skip-gram）**

跳字模型的目标是使用给定的中心词来预测其上下文词汇。具体来说，它通过最大化中心词和其上下文词的共现概率（co-concurrence）来学习词向量。

**公式**

对于一个给定的词语序列w1,w2,…,wT，Skip-gram模型的目标是最大化以下似然函数：

其中，wtw_twt 是中心词，wt+jw_{t+j}wt+j 是其上下文词，ccc 是上下文窗口的大小。

**实现**

Skip-gram模型通过一个简单的神经网络来实现：

**输入层**：一个one-hot编码的词向量。

**隐藏层**：一个线性层，将输入词向量映射到一个低维向量空间。

**输出层**：一个softmax层，输出上下文词的概率分布。

<font style="color:#3370FF;">2. </font>**连续词袋模型（CBOW）**

CBOW模型的目标是使用上下文词汇来预测中心词。它通过最大化上下文词和中心词的共现概率来学习词向量。

**公式**

对于一个给定的词语序列w1,w2,…,wT，CBOW模型的目标是最大化以下似然函数：

**实现**

CBOW模型通过一个简单的神经网络来实现：

**输入层**：多个one-hot编码的上下文词向量。

**隐藏层**：一个线性层，将输入词向量的平均值映射到一个低维向量空间。

**输出层**：一个softmax层，输出中心词的概率分布。

**训练技巧**

为了有效地训练Word2Vec模型，一些优化和技巧通常会被使用：

**负采样（Negative Sampling）**：为了加速训练过程，引入了负采样技术，它通过仅更新部分负样本来简化softmax计算。

**层次softmax（Hierarchical Softmax）**：使用Huffman树来表示softmax层，进一步加速计算。

**子采样（Subsampling）**：对高频词进行欠采样，以减少计算量和提高模型性能。

Word2Vec通过学习将词语映射到低维向量空间，使得具有相似语义的词语在向量空间中彼此接近。Skip-gram模型通过预测上下文词汇来学习中心词的表示，而CBOW模型通过预测中心词来学习上下文词汇的表示。通过使用负采样和层次softmax等优化技术，Word2Vec能够高效地训练并生成高质量的词向量。

**参考文献**

1.Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space.

2.Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems (NIPS).

