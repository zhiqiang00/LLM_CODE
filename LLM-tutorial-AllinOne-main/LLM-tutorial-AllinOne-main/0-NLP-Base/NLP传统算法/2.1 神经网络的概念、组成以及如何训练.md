**2.1 神经网络的概念、组成以及如何训练**

**神经网络的概念**

神经网络（Neural Network）是一种受生物神经系统启发的计算模型，旨在通过层层连接的神经元（节点）来处理和分析数据。每个神经元接收输入信号，进行加权求和，并通过激活函数产生输出信号。神经网络通过调整连接权重，利用反向传播算法进行训练，以最小化预测误差。这种架构能够捕捉复杂的非线性关系，被广泛应用于图像识别、自然语言处理、语音识别等领域。深度神经网络（Deep Neural Network）通过增加层数进一步提升了模型的表达能力和性能。

**神经网络的组成**

<font style="color:#D83931;">神经元（Neurons）</font>：也称为节点或单元，是神经网络的基本计算单元。每个神经元接收输入信号，通过加权求和和激活函数产生输出信号。

<font style="color:#D83931;">层（Layers）</font>：输入层（Input Layer）接收外部数据输入的神经元集合，输入层的神经元数目通常与输入数据的特征数目相同。隐藏层（Hidden Layers）位于输入层和输出层之间，负责对输入数据进行特征提取和转换，隐藏层可以有多层（深度神经网络），每层包含多个神经元。输出层（Output Layer）生成最终预测结果的神经元集合。输出层的神经元数目取决于具体任务，如分类问题中类的数目。

<font style="color:#D83931;">权重（Weights）</font>：表示神经元之间连接的强度。每个连接都有一个权重，表示输入信号的重要性。权重是通过训练过程调整的关键参数。

<font style="color:#D83931;">偏置（Biases）</font>：每个神经元都有一个偏置项，用于调整激活函数的输入，从而控制输出信号。偏置有助于模型在没有输入信号时也能产生非零输出。

<font style="color:#D83931;">激活函数（Activation Functions）</font>：将神经元的加权和输入转换为输出信号的函数，增加网络的非线性表达能力。常见的激活函数有：

Sigmoid：将输入映射到 (0, 1) 区间。

Tanh：将输入映射到 (-1, 1) 区间。

ReLU（Rectified Linear Unit）：将负输入映射为 0，正输入不变。

<font style="color:#D83931;">损失函数（Loss Function）</font>：衡量神经网络预测输出与实际目标之间差异的函数。损失函数是优化过程的核心，常见的损失函数有均方误差（MSE）、交叉熵（Cross Entropy）损失等。

<font style="color:#D83931;">优化器（Optimizers）</font>：用于调整神经网络权重和偏置的算法，以最小化损失函数。常见的优化器包括梯度下降（Gradient Descent）、随机梯度下降（SGD）、Adam、RMSprop等。

<font style="color:#D83931;">前向传播（Forward Propagation）</font>：数据从输入层经过隐藏层到输出层的过程。每层的神经元将输入信号进行加权求和，通过激活函数生成输出信号，传递到下一层。

<font style="color:#D83931;">反向传播（Backward Propagation）</font>：通过计算损失函数相对于每个权重和偏置的梯度，逐层调整网络参数的过程。反向传播结合优化器，使得神经网络能够学习和改进。

<font style="color:#D83931;">正则化（Regularization）</font>：防止过拟合的一些技术，如L1和L2正则化、Dropout等，通过增加模型泛化能力提高性能。

**神经网络的训练**

下面是在MINST图像分类数据集上训练一个简单的多层神经网络的代码

| <font style="color:rgb(100, 106, 115);">Python</font>import torch   from torch import nn, optim   from torchvision import datasets, transforms   from torch.utils.data import DataLoader      # 数据预处理和加载器   transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])   train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)   test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)      train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)   test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)      # 模型定义   class SimpleNN(nn.Module):       def __init__(self):           super(SimpleNN, self).__init__()           self.fc1 = nn.Linear(28*28, 128)           self.fc2 = nn.Linear(128, 64)           self.fc3 = nn.Linear(64, 10)          def forward(self, x):           x = x.view(-1, 28*28)  # 将图像展平成一维向量           x = torch.relu(self.fc1(x))           x = torch.relu(self.fc2(x))           x = self.fc3(x)           return x      model = SimpleNN()      # 损失函数和优化器   criterion = nn.CrossEntropyLoss()   optimizer = optim.Adam(model.parameters(), lr=0.001)      # 训练   def train(model, train_loader, criterion, optimizer, epochs=10):       for epoch in range(epochs):           running_loss = 0.0           for images, labels in train_loader:               optimizer.zero_grad()               outputs = model(images)               loss = criterion(outputs, labels)               loss.backward()               optimizer.step()               running_loss += loss.item()           print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')      # 推理   def evaluate(model, test_loader):       model.eval()       correct = 0       total = 0       with torch.no_grad():           for images, labels in test_loader:               outputs = model(images)               _, predicted = torch.max(outputs.data, 1)               total += labels.size(0)               correct += (predicted == labels).sum().item()       accuracy = 100 * correct / total       print(f'Test Accuracy: {accuracy:.2f}%')      # 训练和评估模型   train(model, train_loader, criterion, optimizer, epochs=10)   evaluate(model, test_loader) |
| :--- |


